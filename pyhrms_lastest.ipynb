{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pymzml\n",
    "import scipy.signal\n",
    "from numpy import *\n",
    "from numba import jit\n",
    "from numba.typed import List\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy.interpolate as interpolate\n",
    "import multiprocessing as mp\n",
    "from molmass import Formula\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "def peak_picking(df1, ms_error=50, threshold=15):\n",
    "    '''\n",
    "    Perform peak picking for a whole LC-MS file, and return the result.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param ms_error: The ms difference between two selected masses (for extraction), this parameter may not affect the final result, but 50 is recommended.\n",
    "    :param threshold: This parameter is used for the function of peak_finding(eic, threhold)\n",
    "    :return:\n",
    "    '''\n",
    "    index = ms_locator(df1, ms_error)  ### 获得ms locator\n",
    "    start_t = time.time()\n",
    "    RT = np.array(df1.columns)\n",
    "    l = len(index)\n",
    "    num = 0\n",
    "    num_p = 0\n",
    "    for i in range(l - 1):\n",
    "        df2 = df1.iloc[index[i]:index[i + 1]]\n",
    "        a = np.array(df2).T  ### 将dataframe转换成np.array\n",
    "        if len(a[0]) != 0:  ### 判断切片结果是否为0\n",
    "            extract_c = a.sum(axis=1)\n",
    "            peak_index, left, right = peak_finding(extract_c, threshold)  ## 关键函数，峰提取\n",
    "            if len(peak_index) != 0:  ### 判断是否找到峰\n",
    "                df3 = df2[df2.columns[peak_index]]\n",
    "                rt = np.round(RT[peak_index], 2)\n",
    "                intensity = np.round(np.array(df3.max().values), 0)\n",
    "                mz = np.round(np.array(df3.idxmax().values), 4)\n",
    "                name = 'peak' + str(num_p)\n",
    "                locals()[name] = np.array([rt, mz, intensity]).T\n",
    "                num_p += 1\n",
    "        p = round(num / l * 100, 1)\n",
    "        print(f' \\r finding peaks...{p}%                   ', end='')  ### 可以切换成百分比\n",
    "        num += 1\n",
    "    data = []\n",
    "    for i in range(num_p - 1):\n",
    "        name = 'peak' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    peak_info = np.concatenate(data)\n",
    "    peak_info_df = pd.DataFrame(data=peak_info, columns=['rt', 'mz', 'intensity'])\n",
    "    return peak_info_df\n",
    "\n",
    "\n",
    "def ms_locator(df1, ppm=50):\n",
    "    '''\n",
    "    For pick picking, selecting a series of mass locators for 50-1000.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param ppm: the mass difference between two locators\n",
    "    :return: mass locators\n",
    "    '''\n",
    "    @jit(nopython=True)\n",
    "    def find_locator(list1, error):\n",
    "        locators = []\n",
    "        locator = list1[0]\n",
    "        for i in range(len(list1)):\n",
    "            if list1[i] > locator:\n",
    "                locators.append(i)\n",
    "                locator *= (1 + error * 1e-6)\n",
    "        return locators\n",
    "\n",
    "    ms_list = list(df1.index)\n",
    "    typed_a = List()\n",
    "    [typed_a.append(x) for x in ms_list]\n",
    "    locators = find_locator(typed_a, ppm)\n",
    "    return locators\n",
    "\n",
    "\n",
    "def sep_scans(path, company):\n",
    "    '''\n",
    "    To separate scan for MS1, MS2 and lockspray. Only supported for Waters .raw and Agilent .d file\n",
    "    :param path: The path for mzML files\n",
    "    :return: ms1, ms2 and lockspray\n",
    "    '''\n",
    "    if company == 'Waters':\n",
    "        a = time.time()\n",
    "        print('\\r Reading files...             ', end=\"\")\n",
    "        run = pymzml.run.Reader(path)\n",
    "        ms1, ms2 = [], []\n",
    "        lockspray = []\n",
    "        for scan in run:\n",
    "            if scan.id_dict['function'] == 1:\n",
    "                ms1.append(scan)\n",
    "            if scan.id_dict['function'] == 2:\n",
    "                ms2.append(scan)\n",
    "            if scan.id_dict['function'] == 3:\n",
    "                lockspray.append(scan)\n",
    "        b = time.time()\n",
    "        time1 = round(b - a, 2)\n",
    "        print(f'\\r Reading files finished! Total time: {time1} s           ', end='')\n",
    "        return ms1, ms2, lockspray\n",
    "    elif company == 'Agilent':\n",
    "        a = time.time()\n",
    "        print('\\r Reading files...             ', end=\"\")\n",
    "        run = pymzml.run.Reader(path)\n",
    "        ms1, ms2 = [], []\n",
    "        for i, scan in enumerate(run):\n",
    "            if scan.ms_level == 1:\n",
    "                ms1.append(scan)\n",
    "            else:\n",
    "                ms2.append(scan)\n",
    "        b = time.time()\n",
    "        time1 = round(b - a, 2)\n",
    "        print(f'\\r Reading files finished! Total time: {time1} s           ', end='')\n",
    "        \n",
    "        return ms1, ms2\n",
    "\n",
    "\n",
    "def peak_finding(eic, threshold=15):\n",
    "    '''\n",
    "    finding peaks in a single extracted chromatogram,and return peak index, left valley index, right valley index.\n",
    "    :param eic: extracted ion chromatogram data; e.g., [1,2,3,2,3,1...]\n",
    "    :param threshold: define the noise level for a peak, 6 is recommend\n",
    "    :return:peak index, left valley index, right valley index.\n",
    "    '''\n",
    "    peaks, _ = scipy.signal.find_peaks(eic, width=2)\n",
    "    prominence = scipy.signal.peak_prominences(eic, peaks)\n",
    "    peak_prominence = prominence[0]\n",
    "    left = prominence[1]\n",
    "    right = prominence[2]\n",
    "    ### peak_picking condition 1: value of peak_prominence must be higher than\n",
    "    len_pro = len(peak_prominence)\n",
    "    if len(peak_prominence) == 0:\n",
    "        peak_index, left, right = np.array([]), np.array([]), np.array([])\n",
    "    else:\n",
    "        median_1 = np.median(peak_prominence)  ### 获得中位数的值\n",
    "        index_pos2 = where(prominence[0] > threshold * median_1)[0]\n",
    "        peak_index = peaks[index_pos2]\n",
    "        left = left[index_pos2]\n",
    "        right = right[index_pos2]\n",
    "    return peak_index, left, right\n",
    "\n",
    "\n",
    "\n",
    "def extract(df1, mz, error=50):\n",
    "    '''\n",
    "    Extracting chromatogram based on mz and error.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param mz: Targeted mass for extraction.\n",
    "    :param error: mass error for extraction\n",
    "    :return: rt,eic\n",
    "    '''\n",
    "    low = mz * (1 - error * 1e-6)\n",
    "    high = mz * (1 + error * 1e-6)\n",
    "    low_index = argmin(abs(df1.index.values - low))\n",
    "    high_index = argmin(abs(df1.index.values - high))\n",
    "    df2 = df1.iloc[low_index:high_index]\n",
    "    rt = df1.columns.values\n",
    "    if len(np.array(df2)) == 0:\n",
    "        intensity = np.zeros(len(df1.columns))\n",
    "    else:\n",
    "        intensity = np.array(df2).T.sum(axis=1)\n",
    "    return rt, intensity  ### 只返回RT和EIC\n",
    "\n",
    "\n",
    "\n",
    "def gen_df_to_centroid(ms1, ms_round=4):\n",
    "    '''\n",
    "    Convert mzml data to a dataframe in centroid mode.\n",
    "    :param ms1: ms scan list generated by the function of sep_scans(), or directed from pymzml.run.Reader(path).\n",
    "    :return: A Dataframe\n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    l = len(ms1)\n",
    "    num = 0\n",
    "    print('\\r Generating dataframe...             ', end=\"\")\n",
    "    ###将所有的数据转换成centroid格式，并将每个scan存在一个独立的变量scan(n)中\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        peaks, _ = scipy.signal.find_peaks(ms1[i].i.copy())\n",
    "        locals()[name] = pd.Series(data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(ms_round),\n",
    "                                   name=round(ms1[i].scan_time[0], 3))\n",
    "        t2 = time.time()\n",
    "        total_t = round(t2 - t1, 2)\n",
    "        p = round(num / l * 100, 2)\n",
    "        print(f'\\r Reading each scans：{total_t} s, {num}/{l}, {p} %     ', end=\"\")\n",
    "        num += 1\n",
    "    ### 将所有的变量汇总到一个列表中\n",
    "    data = []\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    t3 = time.time()\n",
    "    ## 开始级联所有数据\n",
    "    print('\\r Concatenating all the data...                   ', end=\"\")\n",
    "    df1 = pd.concat(data, axis=1)\n",
    "    df2 = df1.fillna(0)\n",
    "    t4 = time.time()\n",
    "    t = round(t4 - t1, 2)\n",
    "    print(f'\\r Concat finished, Consumed time: {t} s            ', end='')\n",
    "    return df2\n",
    "\n",
    "\n",
    "def gen_df_raw(ms1, ms_round=4):\n",
    "    '''\n",
    "    Convert mzml data to a dataframe in profile mode.\n",
    "    :param ms1: ms scan list generated by the function of sep_scans(), or directed from pymzml.run.Reader(path).\n",
    "    :return: A Dataframe\n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    l = len(ms1)\n",
    "    num = 0\n",
    "    print('\\r Generating dataframe...             ', end=\"\")\n",
    "    ###将每个scan存在一个独立的变量scan(n)中\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        locals()[name] = pd.Series(data=ms1[i].i, index=ms1[i].mz.round(ms_round), name=round(ms1[i].scan_time[0], 3))\n",
    "        t2 = time.time()\n",
    "        total_t = round(t2 - t1, 2)\n",
    "        p = round(num / l * 100, 2)\n",
    "        print(f'\\r Reading each scans：{total_t} s, {num}/{l}, {p} %', end=\"\")\n",
    "        num += 1\n",
    "    ### 将所有的变量汇总到一个列表中\n",
    "    data = []\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    t3 = time.time()\n",
    "    ## 开始级联所有数据\n",
    "    print('\\r Concatenating all the data...                             ', end=\"\")\n",
    "    df1 = pd.concat(data, axis=1)\n",
    "    df2 = df1.fillna(0)\n",
    "    t4 = time.time()\n",
    "    t = round(t4 - t1, 2)\n",
    "    print(f'\\r Concat finished, Consumed time: {t} s                     ', end='')\n",
    "    return df2\n",
    "\n",
    "\n",
    "def B_spline(x, y):\n",
    "    '''\n",
    "    Generating more data points for a mass peak using beta-spline based on x,y\n",
    "    :param x: mass coordinates\n",
    "    :param y: intensity\n",
    "    :return: new mass coordinates, new intensity\n",
    "    '''\n",
    "    t, c, k = interpolate.splrep(x, y, s=0, k=4)\n",
    "    N = 300\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    new_x = np.linspace(xmin, xmax, N)\n",
    "    spline = interpolate.BSpline(t, c, k, extrapolate=False)\n",
    "    return new_x, spline(new_x)\n",
    "\n",
    "\n",
    "def cal_bg(data):\n",
    "    '''\n",
    "    :param data: data need to calculate the background\n",
    "    :return: background value\n",
    "    '''\n",
    "    if len(data) > 5:\n",
    "        Median = median(data)\n",
    "        Max_value = max(data)\n",
    "        STD = std(data)\n",
    "        Mean = mean(data)\n",
    "        if Median == 0:\n",
    "            bg = Mean + STD\n",
    "        elif Mean <= Median * 5:\n",
    "            bg = Max_value\n",
    "        elif Mean > Median * 5:\n",
    "            bg = Median\n",
    "    else:\n",
    "        bg = 1000000\n",
    "    return bg + 1\n",
    "\n",
    "\n",
    "def peak_checking_plot(df1, mz, rt1, Type='profile', path=None):\n",
    "    '''\n",
    "    Evaluating/visulizing the extracted mz\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param mz: Targetd mass for extraction\n",
    "    :param rt1: expected rt for peaks\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ### 检查色谱图ax\n",
    "    ax = fig.add_subplot(121)\n",
    "    rt, eic = extract(df1, mz, 50)\n",
    "    rt2 = rt[where((rt > rt1 - 2) & (rt < rt1 + 2))]\n",
    "    eic2 = eic[where((rt > rt1 - 2) & (rt < rt1 + 2))]\n",
    "    ax.plot(rt2, eic2)\n",
    "    ax.set_xlabel('Retention Time(min)', fontsize=12)\n",
    "    ax.set_ylabel('Intensity', fontsize=12)\n",
    "    peak_index = np.argmin(abs(rt - rt1))\n",
    "    peak_height = max(eic[peak_index - 2:peak_index + 2])\n",
    "    ax.scatter(rt1, peak_height * 1.05, c='r', marker='*', s=50)\n",
    "    ##计算背景\n",
    "    bg_left, bg_right = cal_bg(eic2[:50]), cal_bg(eic2[-50:])\n",
    "    rt3 = rt2[:50]\n",
    "    rt4 = rt2[-50:]\n",
    "    bg1 = zeros(50) + bg_left\n",
    "    bg2 = zeros(50) + bg_right\n",
    "    ax.plot(rt3, bg1)\n",
    "    ax.plot(rt4, bg2)\n",
    "    SN1 = round(peak_height / bg_left, 1)\n",
    "    SN2 = round(peak_height / bg_right, 1)\n",
    "    ax.set_title(f'SN_left:{SN1},         SN_right:{SN2}')\n",
    "    ax.set_ylim(top=peak_height * 1.1, bottom=-peak_height * 0.05)\n",
    "\n",
    "    ### 检查质谱图ax1\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    width = 0.02\n",
    "    spec = spec_at_rt(df1,rt1)  ## 提取到特定时间点的质谱图\n",
    "    new_spec = target_spec(spec, mz, width=0.04)\n",
    "    \n",
    "    if Type == 'profile':\n",
    "        mz_obs, error1, mz_opt, error2, resolution = evaluate_ms(new_spec, mz)\n",
    "        ax1.plot(new_spec)\n",
    "        ax1.bar(mz, max(new_spec.values), color='r', width=0.0005)\n",
    "        ax1.bar(mz_opt,max(new_spec.values), color='g', width=0.0005)\n",
    "        ax1.text(min(new_spec.index.values)+0.005, max(new_spec.values)*0.8, \n",
    "             f'mz_obs: {mz_obs},{error1} \\n mz_opt:{mz_opt}, {error2}')\n",
    "    else:\n",
    "        ax1.bar(mz, max(new_spec.values), width=0.0002)\n",
    "\n",
    "    \n",
    "    ax1.set_title(f'mz_exp: {mz}')\n",
    "    ax1.set_xlabel('m/z', fontsize=12)\n",
    "    ax1.set_ylabel('Intensity', fontsize=12)\n",
    "    ax1.set_xlim(mz - 0.04, mz + 0.04)\n",
    "\n",
    "    if path == None:\n",
    "        pass\n",
    "    else:\n",
    "        plt.savefig(path, dpi=1000)\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def peak_alignment(files_excel, rt_error=0.1, mz_error=0.015):\n",
    "    '''\n",
    "    Generating peaks information with reference mz/rt pair\n",
    "    :param files_excel: files for excels of peak picking and peak checking;\n",
    "    :param rt_error: rt error for merge\n",
    "    :param mz_error: mz error for merge\n",
    "    :return: Export to excel files\n",
    "    '''\n",
    "    print('\\r Generating peak reference...        ', end='')\n",
    "    peak_ref = gen_ref(files_excel, rt_error=rt_error, mz_error=mz_error)\n",
    "    j = 1\n",
    "    for file in files_excel:\n",
    "        print(f'\\r for {j} files, generating alignment results...', end='')\n",
    "        peak_p = pd.read_excel(file, index_col='Unnamed: 0').loc[:, ['rt', 'mz']].values\n",
    "        peak_df = pd.read_excel(file, index_col='Unnamed: 0')\n",
    "        new_all_index = []\n",
    "        for i in range(len(peak_p)):\n",
    "            rt1, mz1 = peak_p[i]\n",
    "            index = np.where((peak_ref[:, 0] <= rt1 + rt_error) & (peak_ref[:, 0] >= rt1 - rt_error)\n",
    "                             & (peak_ref[:, 1] <= mz1 + mz_error) & (peak_ref[:, 1] >= mz1 - mz_error))\n",
    "            new_index = str(peak_ref[index][0][0]) + '_' + str(peak_ref[index][0][1])\n",
    "            new_all_index.append(new_index)\n",
    "        peak_df['new_index'] = new_all_index\n",
    "        peak_df = peak_df.set_index('new_index')\n",
    "        peak_df.to_excel(file.replace('.xlsx', '_alignment.xlsx'))\n",
    "        j += 1\n",
    "\n",
    "\n",
    "def database_evaluation(database, i, df1, df2, path=None):\n",
    "    '''\n",
    "    :param database: excel file containing compounds' information\n",
    "    :param i:  the index for a row in excel\n",
    "    :param df1: ms1 dataframe\n",
    "    :param df2: ms2 dataframe\n",
    "    :return:\n",
    "    '''\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    formula = database.loc[i, 'Molecular Formula']\n",
    "    rt_exp = database.loc[i, 'rt']\n",
    "    mz_exp = round(database.loc[i, 'Positive'], 4)\n",
    "    f = Formula(formula)\n",
    "    a = f.spectrum()\n",
    "    mz_iso, i_iso = np.array([a for a in a.values()]).T\n",
    "    i_iso = i_iso / i_iso[0] * 100\n",
    "    mz_iso += 1.00727647  ## 加个H\n",
    "    frag_exp = float(database.loc[i, 'Fragment1'])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    ### 1. 对MS1进行色谱峰提取\n",
    "    ax1 = fig.add_subplot(231)\n",
    "    ax1.set_title('MS1 extracted chromatogram', fontsize=10)\n",
    "    rt1, eic1 = extract(df1, mz_exp, 50)\n",
    "    ax1.plot(rt1, eic1)\n",
    "\n",
    "    ax1.set_ylabel('Intensity', fontsize=12)\n",
    "    peak_index = np.argmin(abs(rt1 - rt_exp))\n",
    "    rt_obs = round(rt1[peak_index - 20:peak_index + 20][np.argmax(eic1[peak_index - 20:peak_index + 20])], 2)  ###找到峰的位置\n",
    "\n",
    "    peak_height = max(eic1[peak_index - 2:peak_index + 2])\n",
    "    ax1.scatter(rt_exp, peak_height * 1.05, c='r', marker='*', s=50)\n",
    "    left1 = rt_exp - 1\n",
    "    right1 = rt_exp + 1\n",
    "\n",
    "    ax1.text(rt_obs + 0.05, peak_height * 0.7, f'RT_obs:{rt_obs} min \\n RT_exp:{rt_exp} min')\n",
    "\n",
    "    ax1.set_xlim(left=left1, right=right1)\n",
    "    ax1.set_ylim(top=peak_height * 1.1, bottom=-peak_height * 0.05)\n",
    "\n",
    "    ### 2. MS1质谱图质量评估\n",
    "    ax2 = fig.add_subplot(232)\n",
    "    ax2.set_title('Isotope check/MS1', fontsize=10)\n",
    "    ind = np.argmin(abs(df1.columns.values - rt_exp))\n",
    "    mz2, i2 = df1.iloc[:, ind].index, df1.iloc[:, ind].values\n",
    "\n",
    "    peaks, _ = scipy.signal.find_peaks(i2)\n",
    "    index1 = peaks[np.argmin(abs(mz2[peaks] - (mz_exp - 1)))]\n",
    "    index2 = peaks[np.argmin(abs(mz2[peaks] - (mz_exp + 5)))]\n",
    "    index_obs = peaks[np.argmin(abs(mz2[peaks] - mz_exp))]\n",
    "    mz_obs = mz2[index_obs]\n",
    "    height_obs = i2[index_obs]  ## 一定要放在之前\n",
    "    mz2, i2 = mz2[index1:index2], i2[index1:index2]\n",
    "\n",
    "    ax2.plot(mz2, i2)\n",
    "    ax2.bar(mz_iso, -(height_obs / 100) * i_iso, width=0.03, color=['r'], zorder=2)\n",
    "    ax2.text(mz_iso[0], -(height_obs / 100) * i_iso[0], round(mz_iso[0], 4))\n",
    "    ax2.text(mz_iso[1], -(height_obs / 100) * i_iso[1], round(mz_iso[1], 4))\n",
    "    ax2.text(mz_iso[2], -(height_obs / 100) * i_iso[2], round(mz_iso[2], 4))\n",
    "\n",
    "    ### 3. 检查MS1质谱准确性\n",
    "    ax3 = fig.add_subplot(233)\n",
    "    ax3.set_title('Accurate mz check/MS1', fontsize=10)\n",
    "    index3 = argmin(abs(mz2 - mz_exp)) - 12\n",
    "    index4 = argmin(abs(mz2 - mz_exp)) + 12\n",
    "    mz3, i3 = mz2[index3:index4], i2[index3:index4]\n",
    "    mz3_opt, i3_opt = B_spline(mz3, i3)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax3.plot(mz3, i3, marker='o')  ### 原始数据\n",
    "    ax3.plot(mz3_opt, i3_opt, c='r', lw=0.5)  ### 优化数据\n",
    "    ax3.bar(mz_exp, height_obs, width=0.001, color=['g', 'b'])  ## 理论质量\n",
    "    obs_error = round((mz_obs - mz_exp) / mz_exp * 1000000, 1)\n",
    "    opt_error = round((mz_opt - mz_exp) / mz_exp * 1000000, 1)\n",
    "    ax3.text(mz_obs - 0.05, height_obs * 0.8,\n",
    "             f'  obs: {round(mz_obs, 4)} \\n  obs_err: {obs_error} \\n  opt: {round(mz_opt, 4)} \\n  opt_err: {opt_error}')\n",
    "    ax3.text(mz_obs + 0.01, height_obs * 1, f'exp: {mz_exp}')\n",
    "\n",
    "    ## 4. 检查DFI 提取色谱图\n",
    "    if np.isnan(frag_exp):\n",
    "        print(f'frag_exp is not given, current frag_exp: {frag_exp}')\n",
    "    else:\n",
    "        ax4 = fig.add_subplot(234)\n",
    "        ax4.set_title('mass spectrum/DFI', fontsize=10)\n",
    "        ax4.set_xlabel('Retention Time(min)', fontsize=12)\n",
    "\n",
    "        rt4, eic4 = extract(df2, frag_exp, 50)  ##提取改成df2\n",
    "        ax4.plot(rt4, eic4)\n",
    "        ax4.set_ylabel('Intensity', fontsize=12)\n",
    "        peak_index = np.argmin(abs(rt4 - rt_exp))\n",
    "        peak_height4 = max(eic4[peak_index - 2:peak_index + 2])\n",
    "        ax4.scatter(rt_exp, peak_height4 * 1.05, c='r', marker='*', s=50)\n",
    "        left4 = rt_exp - 1\n",
    "        right4 = rt_exp + 1\n",
    "        ax4.set_xlim(left=left4, right=right4)\n",
    "        ax4.text(rt_exp + 0.05, peak_height4 * 0.7, f' RT_exp:{rt_exp} min')\n",
    "        ax4.set_ylim(top=peak_height4 * 1.1, bottom=-peak_height4 * 0.05)\n",
    "\n",
    "        ### 5. 检查DFI同位素峰\n",
    "        ax5 = fig.add_subplot(235)\n",
    "        ax5.set_title('DFI check', fontsize=10)\n",
    "        ax5.set_xlabel('m/z', fontsize=12)\n",
    "        index5 = np.argmin(abs(df2.columns.values - rt_obs))\n",
    "        mz5, i5 = df2.iloc[:, index5].index.values, df2.iloc[:, index5].values\n",
    "        index5_1 = np.argmin(abs(mz5 - frag_exp))\n",
    "        height_obs = i5[index5_1]\n",
    "        index5_2 = np.argmin(abs(mz5 - (frag_exp - 1)))\n",
    "        index5_3 = np.argmin(abs(mz5 - (frag_exp + 4)))\n",
    "        mz5, i5 = mz5[index5_2:index5_3], i5[index5_2:index5_3]\n",
    "        ax5.plot(mz5, i5)\n",
    "        frag_iso = mz_iso - (mz_iso - frag_exp)[0]\n",
    "        ax5.bar(frag_iso + 0.1, (height_obs / 100) * i_iso, width=0.03, color=['m'], zorder=2)\n",
    "        ax5.text(frag_iso[0] + 0.1, (height_obs / 100) * i_iso[0], round(frag_iso[0], 4))\n",
    "        ax5.text(frag_iso[1] + 0.1, (height_obs / 100) * i_iso[1], round(frag_iso[1], 4))\n",
    "        ax5.text(frag_iso[2] + 0.1, (height_obs / 100) * i_iso[2], round(frag_iso[2], 4))\n",
    "\n",
    "        ### 6. 检查DFI质谱峰\n",
    "        ax6 = fig.add_subplot(236)\n",
    "        ax6.set_title('Accurate DFI check', fontsize=10)\n",
    "        ax6.set_xlabel('m/z', fontsize=12)\n",
    "        index6_1 = argmin(abs(mz5 - frag_exp)) - 12\n",
    "        index6_2 = argmin(abs(mz5 - frag_exp)) + 12\n",
    "        mz6, i6 = mz5[index6_1:index6_2], i5[index6_1:index6_2]\n",
    "        frag_obs = round(mz6[np.argmax(i6)], 4)\n",
    "        mz6_opt, i6_opt = B_spline(mz6, i6)\n",
    "        frag_opt = mz6_opt[np.argmax(i6_opt)]\n",
    "\n",
    "        ax6.plot(mz6, i6, marker='o')  ### 原始数据\n",
    "        ax6.plot(mz6_opt, i6_opt, c='r', lw=0.5)  ### 优化数据\n",
    "        ax6.bar(frag_exp, height_obs, width=0.001, color=['g'])\n",
    "\n",
    "        obs_error = round((frag_obs - frag_exp) / mz_exp * 1000000, 1)\n",
    "        opt_error = round((frag_opt - frag_exp) / mz_exp * 1000000, 1)\n",
    "        ax6.text(frag_obs - 0.04, height_obs * 0.8,\n",
    "                 f'  obs: {round(frag_obs, 4)} \\n  obs_err: {obs_error} \\n  opt: {round(frag_opt, 4)} \\n  opt_err: {opt_error}')\n",
    "        ax6.text(frag_obs + 0.01, height_obs * 1, f'exp: {frag_exp}')\n",
    "        \n",
    "    if path ==None:\n",
    "        pass\n",
    "    else:   \n",
    "        fig.savefig(path, dpi=1000)\n",
    "        plt.close(fig)\n",
    "\n",
    "def peak_checking(peak_df, df1, error=50,\n",
    "                  i_threshold=500, SN_threshold=5):\n",
    "    '''\n",
    "    Processing extracted peaks, remove those false positives.\n",
    "    :param peak_df: Extracted peaks generated by the function of peak_picking\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param error: For the function of extract(df,mz, error)\n",
    "    :param i_threshold: filter peaks with intensity<i_threshold\n",
    "    :param SN_threshold: filter peaks with sn<SN_threshold\n",
    "    :return:\n",
    "    '''\n",
    "    final_result = pd.DataFrame()\n",
    "    n = 0\n",
    "    peak_num = len(peak_df['rt'])\n",
    "    SN_all_left, SN_all_right, area_all, cor_all_mz, cor_all_i = [], [], [], [], []\n",
    "    for i in range(peak_num):\n",
    "        mz = peak_df.iloc[i]['mz']\n",
    "        rt = peak_df.iloc[i]['rt']\n",
    "        ### 第一步：处理色谱峰\n",
    "        rt_e, eic_e = extract(df1, mz, error=error)\n",
    "        peak_index = np.argmin(abs(rt_e - rt))  ## 找到特定时间点的索引\n",
    "        rt_left = rt - 0.2\n",
    "        rt_right = rt + 0.2\n",
    "        peak_index_left = np.argmin(abs(rt_e - rt_left))\n",
    "        peak_index_right = np.argmin(abs(rt_e - rt_right))\n",
    "        mz_all, intensity_t = df1.iloc[:, peak_index].index.values, df1.iloc[:, peak_index].values  ## 提取特定时间的质谱峰\n",
    "        try:\n",
    "            peak_height = max(eic_e[peak_index - 2:peak_index + 2])\n",
    "            other_peak = max(eic_e[peak_index - 5:peak_index + 5])\n",
    "        except:\n",
    "            peak_height = 1\n",
    "            other_peak = 3\n",
    "        rt_t, eic_t = rt_e[peak_index_left:peak_index_right], eic_e[peak_index_left:peak_index_right]\n",
    "        try:\n",
    "            area = scipy.integrate.simps(eic_e[peak_index - 40:peak_index + 40])\n",
    "        except:\n",
    "            area = scipy.integrate.simps(eic_e)\n",
    "        if other_peak - peak_height > 1:\n",
    "            bg_left, bg_right = 10000000, 10000000\n",
    "        else:\n",
    "            bg_left = cal_bg(eic_t[:50])\n",
    "            bg_right = cal_bg(eic_t[-50:])\n",
    "\n",
    "        SN_left = round(peak_height / bg_left, 1)\n",
    "        SN_right = round(peak_height / bg_right, 1)\n",
    "        SN_all_left.append(SN_left)\n",
    "        SN_all_right.append(SN_right)\n",
    "        area_all.append(area)\n",
    "\n",
    "        ### 第二步：处理质谱峰\n",
    "        mz_l, mz_h = mz - 0.005, mz + 0.005\n",
    "        mz_all, intensity_t = df1.iloc[:, peak_index].index.values, df1.iloc[:, peak_index].values\n",
    "        mz_index = argmin(abs(mz_all - mz))\n",
    "        mz_width = 20\n",
    "        mz_, i_ = mz_all[mz_index - mz_width:mz_index + mz_width], intensity_t[mz_index - mz_width:mz_index + mz_width]\n",
    "        peaks, _ = scipy.signal.find_peaks(i_)\n",
    "        peak_mz, peak_i = mz_[peaks], i_[peaks]\n",
    "        peak_mz, peak_i = peak_mz[(peak_mz >= mz_l) & (peak_mz <= mz_h)], peak_i[(peak_mz >= mz_l) & (peak_mz <= mz_h)]\n",
    "        if len(peak_mz) == 0:\n",
    "            cor_mz, cor_i = 0, 1\n",
    "        else:\n",
    "            cor_mz = round(peak_mz[argmax(peak_i)], 4)\n",
    "            cor_i = round(peak_i[argmax(peak_i)], 0)\n",
    "        cor_all_mz.append(cor_mz)\n",
    "        cor_all_i.append(cor_i)\n",
    "        n += 1\n",
    "        print(f'\\r Processing peaks...{n}/{peak_num}                        ', end='')\n",
    "    final_result['SN_left'] = SN_all_left\n",
    "    final_result['SN_right'] = SN_all_right\n",
    "    final_result['area'] = list(map(int, area_all))\n",
    "    final_result['mz'] = cor_all_mz\n",
    "    final_result['intensity'] = cor_all_i\n",
    "    final_result['rt'] = peak_df['rt']\n",
    "    ### 筛选条件，峰强度> i_threshold; 左边和右边SN至少一个大于SN_threshold\n",
    "    final_result = final_result[(final_result['intensity'] > i_threshold) &\n",
    "                                ((final_result['SN_left'] > SN_threshold) | (final_result['SN_right'] > SN_threshold))]\n",
    "    final_result = final_result.loc[:, ['rt', 'mz', 'intensity', 'SN_left', 'SN_right', 'area']].sort_values(\n",
    "        by='intensity').reset_index(drop=True)\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def spec_at_rt(df1, rt):\n",
    "    '''\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param rt:  rentention time for certain ms spec\n",
    "    :return: ms spec\n",
    "    '''\n",
    "    index = argmin(abs(df1.columns.values - rt))\n",
    "    spec = df1.iloc[:, index]\n",
    "    return spec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def concat_alignment(files_excel):\n",
    "    '''\n",
    "    Concatenate all data and return\n",
    "    :param files_excel: excel files\n",
    "    :param mode: selected 'area' or 'intensity' for each sample\n",
    "    :return: dataframe\n",
    "    '''\n",
    "    align = []\n",
    "    data_to_concat = []\n",
    "    for i in range(len(files_excel)):\n",
    "        if 'alignment' in files_excel[i]:\n",
    "            align.append(files_excel[i])\n",
    "    for i in range(len(align)):\n",
    "        name = 'data' + str(i)\n",
    "        locals()[name] = pd.read_excel(align[i], index_col='Unnamed: 0')\n",
    "        data_to_concat.append(locals()[name])\n",
    "        print(f'\\r {i}/{len(align)}',end='            ')\n",
    "    final_data = pd.concat(data_to_concat, axis=1)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "\n",
    "def formula_to_distribution(formula, adducts='+H', num=3):\n",
    "    '''\n",
    "    :param formula: molecular formula, e.g., ‘C13H13N3’\n",
    "    :param adducts: ion adducts, '+H', '-H'\n",
    "    :return: mz_iso, i_iso (np.array)\n",
    "    '''\n",
    "    f = Formula(formula)\n",
    "    a = f.spectrum()\n",
    "    mz_iso, i_iso = np.array([a for a in a.values()]).T\n",
    "    i_iso = i_iso / i_iso[0] * 100\n",
    "    if adducts == '+H':\n",
    "        mz_iso += 1.00727647\n",
    "    elif adducts == '-H':\n",
    "        mz_iso -= 1.00727647\n",
    "    mz_iso = mz_iso.round(4)\n",
    "    i_iso = i_iso.round(1)\n",
    "    return mz_iso[:num], i_iso[:num]\n",
    "\n",
    "\n",
    "def add_ms_values(new_spec):\n",
    "    '''\n",
    "    :param new_spec: the spectrum (pandas.Series)\n",
    "    :return:  peak_mz,peak_i\n",
    "    '''\n",
    "    peaks, _ = scipy.signal.find_peaks(new_spec.values)\n",
    "    peak3 = new_spec.iloc[peaks].sort_values().iloc[-3:]\n",
    "    peak_mz = peak3.index.values\n",
    "    peak_i = peak3.values\n",
    "    return peak_mz, peak_i\n",
    "\n",
    "\n",
    "def multi_process(file, company):\n",
    "    ms1, *_ = sep_scans(file, company)\n",
    "    df1 = gen_df_raw(ms1)\n",
    "    peak_all = peak_picking(df1)\n",
    "    peak_selected = peak_checking(peak_all, df1)\n",
    "    peak_selected.to_excel(file.replace('.mzML', '.xlsx'))\n",
    "\n",
    "\n",
    "def KMD_cal(mz_set, group='Br/H'):\n",
    "    if '/' in group:\n",
    "        g1, g2 = group.split('/')\n",
    "        f1, f2 = Formula(g1), Formula(g2)\n",
    "        f1, f2 = f1.spectrum(), f2.spectrum()\n",
    "        f1_value, f2_value = [x for x in f1.values()][0][0], [x for x in f2.values()][0][0]\n",
    "        values = [abs(f1_value - f2_value), round(abs(f1_value - f2_value), 0)]\n",
    "        KM = mz_set * (max(values) / min(values))\n",
    "        KMD_set = KM - np.floor(KM)\n",
    "\n",
    "        print(f1_value, f2_value)\n",
    "        print(min(values), max(values))\n",
    "        print(values)\n",
    "    else:\n",
    "        g1 = Formula(group)\n",
    "        f1 = g1.spectrum()\n",
    "        f1_value = [x for x in f1.values()][0][0]\n",
    "        KM = mz_set * (int(f1_value) / f1_value)\n",
    "        KMD_set = KM - np.floor(mz_set)\n",
    "    return KMD_set\n",
    "\n",
    "\n",
    "def sep_result(result, replicate=4, batch=5):\n",
    "    a = 0\n",
    "    sep_result = []\n",
    "    for i in range(batch):\n",
    "        name = 'b' + str(i)\n",
    "        sep_result.append(result[result.columns[a:a + replicate]])\n",
    "        a += replicate\n",
    "\n",
    "    return sep_result\n",
    "\n",
    "\n",
    "\n",
    "def peak_checking_area(ref_all,df1,name='area'):\n",
    "    '''\n",
    "    Based on referece pairs, extract all peaks and integrate the peak area.\n",
    "    :param ref_all: all referece pairs (dataframe)\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param name: name for area\n",
    "    :return: peak_ref (dataframe)\n",
    "    '''\n",
    "    area_all = []\n",
    "    peak_index = np.array(ref_all['rt'].map(lambda x:str(round(x,2))).str.cat(ref_all['mz'].map(lambda x:str(round(x,4))),sep = '_'))\n",
    "    num = len(ref_all)\n",
    "    for i in range(num):\n",
    "        rt,mz =ref_all.loc[i,['rt','mz']]\n",
    "        rt1,eic1 = extract(df1,mz,50)\n",
    "        rt_ind = argmin(abs(rt1-rt))\n",
    "        left = argmin(abs(rt1-(rt-0.2)))\n",
    "        right = argmin(abs(rt1-(rt+0.2)))\n",
    "        rt_t,eic_t = rt1[left:right],eic1[left:right]\n",
    "        area = round(scipy.integrate.simps(eic_t,rt_t),0)\n",
    "        area_all.append(area)\n",
    "        print(f'\\r {i}/{num}', end = '')\n",
    "    peak_ref = pd.DataFrame(area_all,index = peak_index,columns = [name])\n",
    "    return peak_ref\n",
    "\n",
    "\n",
    "def JsonToExcel(path):\n",
    "    with open(path,'r',encoding='utf8')as fp:\n",
    "        json_data = json.load(fp)\n",
    "    Inchikey,precursor,frag,formula,smiles = [],[],[],[],[]\n",
    "    num = len(json_data)\n",
    "    for i in range(num):\n",
    "        try:\n",
    "            cmp_info = json_data[i]['compound'][0]['metaData']\n",
    "            Inchikey.append([x['value'] for x in cmp_info if x['name']=='InChIKey'][0])\n",
    "            formula.append([x['value'] for x in cmp_info if x['name']=='molecular formula'][0])\n",
    "            precursor.append([x['value'] for x in cmp_info if x['name']=='total exact mass'][0])\n",
    "            smiles.append([x['value'] for x in cmp_info if x['name']=='SMILES'][0])\n",
    "        except:\n",
    "            Inchikey.append(None)\n",
    "            formula.append(None)\n",
    "            precursor.append(None)\n",
    "            smiles.append(None)\n",
    "        spec1 = r'{' + json_data[i]['spectrum'].replace(' ',',') + r'}'\n",
    "        spec2 = pd.Series(eval(spec1)).sort_values()\n",
    "        spec3 = spec2[spec2.index[-50:]].to_dict()\n",
    "        frag.append(spec3)    \n",
    "        print(f'\\r {round(i/num*100,2)}%',end='')\n",
    "    database = pd.DataFrame(np.array([Inchikey,precursor,frag,formula,smiles]).T,\n",
    "                            columns = ['Inchikey','Precursor','Frag','Formula','Smiles'])\n",
    "    return database\n",
    "\n",
    "\n",
    "def evaluate_ms(new_spec,mz_exp):\n",
    "    peaks,_ = scipy.signal.find_peaks(new_spec.values)\n",
    "    mz_obs = new_spec.index.values[peaks][argmin(abs(new_spec.index.values[peaks]-mz_exp))]\n",
    "    x, y = B_spline(new_spec.index.values, new_spec.values)\n",
    "    peaks,_ = scipy.signal.find_peaks(y)\n",
    "    max_index = peaks[argmin(abs(x[peaks]-mz_exp))]\n",
    "    half_height = y[max_index]/2\n",
    "    mz_left = x[:max_index][argmin(abs(y[:max_index] - half_height))]\n",
    "    mz_right = x[max_index:][argmin(abs(y[max_index:] - half_height))]\n",
    "    resolution = int(mz_obs / (mz_right - mz_left))\n",
    "    mz_opt = round(mz_left+(mz_right - mz_left)/2, 4)\n",
    "    mz_opt_ref = round(x[max_index],4)\n",
    "    \n",
    "    if abs(mz_opt-mz_opt_ref)/mz_exp*1000000 <10:\n",
    "        final_mz_opt = mz_opt\n",
    "    else:\n",
    "        final_mz_opt = mz_opt_ref\n",
    "        \n",
    "    error1 = round((mz_obs -mz_exp)/mz_exp*1000000,1)\n",
    "    error2 = round((final_mz_opt -mz_exp)/mz_exp*1000000,1)\n",
    "    return mz_obs,error1,final_mz_opt,error2,resolution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def target_spec(spec, target_mz, width=0.04):\n",
    "    '''\n",
    "    :param spec: spec generated from function spec_at_rt()\n",
    "    :param target_mz: target mz for inspection\n",
    "    :param width: width for data points\n",
    "    :return: new spec and observed mz\n",
    "    '''\n",
    "    index = argmin(abs(spec.index.values-target_mz))\n",
    "    index_left =argmin(abs(spec.index.values-(target_mz-width)))\n",
    "    index_right =argmin(abs(spec.index.values-(target_mz+width)))\n",
    "    new_spec = spec.iloc[index_left:index_right]\n",
    "    return new_spec\n",
    "\n",
    "\n",
    "def gen_ref(files_excel, mz_error=0.015, rt_error=0.1):\n",
    "    '''\n",
    "    For alignment, generating a reference mz/rt pair\n",
    "    :param files_excel: excel files path for extracted peaks\n",
    "    :return: mz/rt pair reference\n",
    "    '''\n",
    "    data = []\n",
    "    for i in range(len(files_excel)):\n",
    "        name = 'peaks' + str(i)\n",
    "        locals()[name] = pd.read_excel(files_excel[i], index_col='Unnamed: 0').loc[:, ['rt', 'mz']].values\n",
    "        data.append(locals()[name])\n",
    "        print(f'\\r Reading excel files... {i}/{len(files_excel)}                   ', end=\"\")\n",
    "    print(f'\\r Concatenating all peaks...                 ', end='')\n",
    "    pair = np.concatenate(data, axis=0)\n",
    "    peak_all_check = pair\n",
    "    peak_ref = []\n",
    "    while len(pair) > 0:\n",
    "        rt1, mz1 = pair[0]\n",
    "        index1 = np.where((pair[:, 0] <= rt1 + rt_error) & (pair[:, 0] >= rt1 - rt_error)\n",
    "                          & (pair[:, 1] <= mz1 + mz_error) & (pair[:, 1] >= mz1 - mz_error))\n",
    "        peak = np.mean(pair[index1], axis=0).tolist()\n",
    "        peak = [round(peak[0],2),round(peak[1],4)]\n",
    "        pair = np.delete(pair, index1, axis=0)\n",
    "        peak_ref.append(peak)\n",
    "        print(f'\\r  {len(pair)}                        ', end='')\n",
    "\n",
    "    peak_ref2 = np.array(peak_ref)\n",
    "    \n",
    "    ### 检查是否有漏的\n",
    "    peak_lost = []\n",
    "    for peak in peak_all_check:\n",
    "        rt1, mz1 = peak\n",
    "        check = np.where((peak_ref2[:, 0] <= rt1 + rt_error) & (peak_ref2[:, 0] >= rt1 - rt_error)\n",
    "                         & (peak_ref2[:, 1] <= mz1 + mz_error) & (peak_ref2[:, 1] >= mz1 - mz_error))\n",
    "        if len(check[0]) == 0:\n",
    "            peak_lost.append([rt1, mz1])\n",
    "    peak_lost=np.array(peak_lost)\n",
    "    while len(peak_lost) > 0:\n",
    "        rt1, mz1 = peak_lost[0]\n",
    "        index1 = np.where((peak_lost[:, 0] <= rt1 + rt_error) & (peak_lost[:, 0] >= rt1 - rt_error)\n",
    "                          & (peak_lost[:, 1] <= mz1 + mz_error) & (peak_lost[:, 1] >= mz1 - mz_error))\n",
    "        peak = np.mean(peak_lost[index1], axis=0).tolist()\n",
    "        peak = [round(peak[0],2),round(peak[1],4)]\n",
    "        peak_lost = np.delete(peak_lost, index1, axis=0)\n",
    "        peak_ref.append(peak)\n",
    "        print(f'\\r  {len(pair)}                        ', end='')\n",
    "    \n",
    "    return np.array(peak_ref)\n",
    "\n",
    "def ms_bg_removal(background,target_spec,mz_error = 0.01):\n",
    "    '''\n",
    "    Only support for centroid data, please convert profile data to centroid\n",
    "    '''\n",
    "    target_spec = target_spec[target_spec>100]\n",
    "    bg = []\n",
    "    for i in target_spec.index.values:\n",
    "        index = argmin(abs(background.index.values-i))\n",
    "        if background.index.values[index]-i<mz_error:\n",
    "            bg.append([i,background.values[index]])\n",
    "        else:\n",
    "            bg.append([i,0])\n",
    "    bg_spec = pd.Series(np.array(bg).T[1],np.array(bg).T[0])\n",
    "    spec_bg_removal = target_spec - bg_spec\n",
    "    return spec_bg_removal[spec_bg_removal>100].sort_values()\n",
    "\n",
    "def gen_frag(df1,df2,rt,company = 'Waters',mode = 'profile'):\n",
    "    if company == 'Waters':\n",
    "        if mode == 'profile':\n",
    "            spec1 = ms_to_centroid(spec_at_rt(df1,rt))\n",
    "            spec2 = ms_to_centroid(spec_at_rt(df2,rt))\n",
    "            spec_bg_removal = ms_bg_removal(spec1,spec2)           \n",
    "        elif mode == 'centroid':\n",
    "            spec1 = spec_at_rt(df1,rt)\n",
    "            spec2 = spec_at_rt(df2,rt)\n",
    "            spec_bg_removal = ms_bg_removal(spec1,spec2)        \n",
    "        else:\n",
    "            new_frag = None\n",
    "            print('please check the mode input, only accept profile or centroid')\n",
    "    return spec_bg_removal\n",
    "\n",
    "\n",
    "def ms_to_centroid(spec):\n",
    "    peaks,_ = scipy.signal.find_peaks(spec.values)\n",
    "    new_index = spec.index.values[peaks]\n",
    "    new_values = spec.values[peaks]\n",
    "    new_spec = pd.Series(new_values,new_index)\n",
    "    return new_spec\n",
    "\n",
    "def spec_similarity(spec_obs,suspect_frag,error=0.005):\n",
    "    fragments = suspect_frag.index.values[-10:]\n",
    "    score=0\n",
    "    for i in fragments:\n",
    "        if min(abs(spec_obs.index.values-i))< error:\n",
    "            score+=1\n",
    "    return score/len(fragments)\n",
    "\n",
    "def massbank_match(mz,spec_obs,database1,mode = 'pos',ms1_error = 0.015,ms2_error =0.005,vis = False ):\n",
    "    data ={}\n",
    "    if mode =='pos':\n",
    "        error = abs(database1['Precursor'].values-(mz-1.0078)) \n",
    "        suspect_ind = where(error<ms1_error)[0]\n",
    "    else:\n",
    "        error = abs(database1['Precursor'].values-(mz+1.0078)) \n",
    "        suspect_ind = where(error<ms1_error)[0]\n",
    "    if len(suspect_ind)==0:\n",
    "        msbank_result = pd.DataFrame(data,columns = data.keys(),index = ['Inchikey','score']).T\n",
    "    else:\n",
    "        for num,i in enumerate(suspect_ind):\n",
    "            suspect_frag = pd.Series(eval(database1['Frag'][i])).sort_values()\n",
    "            cmp_info = database1['Inchikey'][i]\n",
    "            score = spec_similarity(spec_obs,suspect_frag,error = ms2_error)\n",
    "            data[num] = [i,cmp_info,score]\n",
    "        msbank_result = pd.DataFrame(data,columns = data.keys(),index = ['index','Inchikey','score']).T\n",
    "        msbank_result = msbank_result.sort_values(by = 'score').reset_index(drop=True)\n",
    "    if vis == True:\n",
    "        if len(msbank_result) !=0:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.bar(spec_obs.index.values,spec_obs.values/max(spec_obs.values)*100)\n",
    "            index = msbank_result['index'].values[-1]\n",
    "            suspect_frag = pd.Series(eval(database1['Frag'][index])).sort_values()\n",
    "            ax.bar(suspect_frag.index.values,-suspect_frag.values)\n",
    "            ax.set_xlabel('m/z')\n",
    "            ax.set_ylabel('Relative intensity')\n",
    "            plt.xlim(50,mz*1.2)\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        pass\n",
    "    return msbank_result\n",
    "\n",
    "\n",
    "def one_step_process(files_mzml,company):\n",
    "    if company == 'Waters':\n",
    "        mz_round = 4\n",
    "    elif company == 'Agilent':\n",
    "        mz_round = 3\n",
    "    else:\n",
    "        print('Error:Only support for Aglient or Waters files')\n",
    "    for file in files_mzml:\n",
    "        ms1,*_ =sep_scans(file,company)\n",
    "        df1 = gen_df_raw(ms1,mz_round)\n",
    "        peak_all = peak_picking(df1)\n",
    "        peak_selected = peak_checking(peak_all,df1)\n",
    "        peak_selected.to_excel(file.replace('.mzML','.xlsx'))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "#     path = r'D:\\TOF-Ms DATA\\HYH-MZML\\混合实验\\*.mzML'\n",
    "#     files_mzml = glob(path)\n",
    "#     pool = Pool(processes = 5)\n",
    "#     for file in files_mzml:\n",
    "#         pool.apply_async(multi_process,args=(file,))\n",
    "#     print('Finished')\n",
    "#     pool.close()\n",
    "#     pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'G:\\广东省重点海域非靶向调查-202109\\LC-2109-pos\\HHW\\mzML\\step2\\*.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_excel = glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53/54            "
     ]
    }
   ],
   "source": [
    "df_result = concat_alignment(files_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "example =df_result[df_result.columns[6:12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = (example+1).apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SW210903HHW-yangpinping-blank-1</th>\n",
       "      <th>SW210903HHW-yangpinping-blank-2</th>\n",
       "      <th>SW210903HHW-yangpinping-blank-3</th>\n",
       "      <th>SW210904HHW01-1</th>\n",
       "      <th>SW210904HHW01-2</th>\n",
       "      <th>SW210904HHW01-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20.22_282.5023</th>\n",
       "      <td>8.313362</td>\n",
       "      <td>8.181720</td>\n",
       "      <td>8.582981</td>\n",
       "      <td>4.158883</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.833213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.22_245.2671</th>\n",
       "      <td>6.082219</td>\n",
       "      <td>5.541264</td>\n",
       "      <td>5.799093</td>\n",
       "      <td>3.258097</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>2.944439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.45_536.3721</th>\n",
       "      <td>5.433722</td>\n",
       "      <td>5.472271</td>\n",
       "      <td>5.501258</td>\n",
       "      <td>5.752573</td>\n",
       "      <td>5.560682</td>\n",
       "      <td>5.497168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22.45_521.1706</th>\n",
       "      <td>5.420535</td>\n",
       "      <td>5.135798</td>\n",
       "      <td>4.927254</td>\n",
       "      <td>6.566672</td>\n",
       "      <td>5.855072</td>\n",
       "      <td>5.541264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.35_155.0139</th>\n",
       "      <td>2.772589</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>4.127134</td>\n",
       "      <td>4.189655</td>\n",
       "      <td>4.025352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12.13_397.8214</th>\n",
       "      <td>5.313206</td>\n",
       "      <td>5.117994</td>\n",
       "      <td>5.438079</td>\n",
       "      <td>4.465908</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>3.555348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17.97_594.1467</th>\n",
       "      <td>8.261010</td>\n",
       "      <td>8.197814</td>\n",
       "      <td>8.079308</td>\n",
       "      <td>7.752765</td>\n",
       "      <td>6.317165</td>\n",
       "      <td>5.283204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.64_414.9326</th>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>7.434848</td>\n",
       "      <td>6.984716</td>\n",
       "      <td>4.927254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.13_426.0982</th>\n",
       "      <td>3.784190</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>3.610918</td>\n",
       "      <td>5.480639</td>\n",
       "      <td>5.278115</td>\n",
       "      <td>5.030438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.75_962.8073</th>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.791759</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>5.710427</td>\n",
       "      <td>5.442418</td>\n",
       "      <td>4.262680</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8392 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                SW210903HHW-yangpinping-blank-1  \\\n",
       "20.22_282.5023                         8.313362   \n",
       "17.22_245.2671                         6.082219   \n",
       "22.45_536.3721                         5.433722   \n",
       "22.45_521.1706                         5.420535   \n",
       "12.35_155.0139                         2.772589   \n",
       "...                                         ...   \n",
       "12.13_397.8214                         5.313206   \n",
       "17.97_594.1467                         8.261010   \n",
       "1.64_414.9326                          2.397895   \n",
       "14.13_426.0982                         3.784190   \n",
       "1.75_962.8073                          2.197225   \n",
       "\n",
       "                SW210903HHW-yangpinping-blank-2  \\\n",
       "20.22_282.5023                         8.181720   \n",
       "17.22_245.2671                         5.541264   \n",
       "22.45_536.3721                         5.472271   \n",
       "22.45_521.1706                         5.135798   \n",
       "12.35_155.0139                         2.708050   \n",
       "...                                         ...   \n",
       "12.13_397.8214                         5.117994   \n",
       "17.97_594.1467                         8.197814   \n",
       "1.64_414.9326                          2.890372   \n",
       "14.13_426.0982                         3.610918   \n",
       "1.75_962.8073                          1.791759   \n",
       "\n",
       "                SW210903HHW-yangpinping-blank-3  SW210904HHW01-1  \\\n",
       "20.22_282.5023                         8.582981         4.158883   \n",
       "17.22_245.2671                         5.799093         3.258097   \n",
       "22.45_536.3721                         5.501258         5.752573   \n",
       "22.45_521.1706                         4.927254         6.566672   \n",
       "12.35_155.0139                         2.302585         4.127134   \n",
       "...                                         ...              ...   \n",
       "12.13_397.8214                         5.438079         4.465908   \n",
       "17.97_594.1467                         8.079308         7.752765   \n",
       "1.64_414.9326                          1.386294         7.434848   \n",
       "14.13_426.0982                         3.610918         5.480639   \n",
       "1.75_962.8073                          1.609438         5.710427   \n",
       "\n",
       "                SW210904HHW01-2  SW210904HHW01-3  \n",
       "20.22_282.5023         2.484907         2.833213  \n",
       "17.22_245.2671         3.637586         2.944439  \n",
       "22.45_536.3721         5.560682         5.497168  \n",
       "22.45_521.1706         5.855072         5.541264  \n",
       "12.35_155.0139         4.189655         4.025352  \n",
       "...                         ...              ...  \n",
       "12.13_397.8214         3.761200         3.555348  \n",
       "17.97_594.1467         6.317165         5.283204  \n",
       "1.64_414.9326          6.984716         4.927254  \n",
       "14.13_426.0982         5.278115         5.030438  \n",
       "1.75_962.8073          5.442418         4.262680  \n",
       "\n",
       "[8392 rows x 6 columns]"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\server\\anaconda3\\lib\\site-packages\\seaborn\\matrix.py:659: UserWarning:\n",
      "\n",
      "Clustering large matrix with scipy. Installing `fastcluster` may give better performance.\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.matrix.ClusterGrid at 0x2df5c5f1d08>"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.clustermap(example1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\server\\Documents\\git\\lib\\*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_json= glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100.0%"
     ]
    }
   ],
   "source": [
    "for i,file in enumerate(files_json[1:]):\n",
    "    data_base = JsonToExcel(file)\n",
    "    data_base.to_excel(file.replace('.json','.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base.to_excel(file.replace('.json','.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
