{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pymzml\n",
    "import scipy.signal\n",
    "from numpy import *\n",
    "from numba import jit\n",
    "from numba.typed import List\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy.interpolate as interpolate\n",
    "import multiprocessing as mp\n",
    "from molmass import Formula\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "\n",
    "\n",
    "# %config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "def peak_picking(df1, ms_error=50, threshold=15):\n",
    "    '''\n",
    "    Perform peak picking for a whole LC-MS file, and return the result.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param ms_error: The ms difference between two selected masses (for extraction), this parameter may not affect the final result, but 50 is recommended.\n",
    "    :param threshold: This parameter is used for the function of peak_finding(eic, threhold)\n",
    "    :return:\n",
    "    '''\n",
    "    index = ms_locator(df1, ms_error)  ### 获得ms locator\n",
    "    start_t = time.time()\n",
    "    RT = np.array(df1.columns)\n",
    "    l = len(index)\n",
    "    num = 0\n",
    "    num_p = 0\n",
    "    for i in range(l - 1):\n",
    "        df2 = df1.iloc[index[i]:index[i + 1]]\n",
    "        a = np.array(df2).T  ### 将dataframe转换成np.array\n",
    "        if len(a[0]) != 0:  ### 判断切片结果是否为0\n",
    "            extract_c = a.sum(axis=1)\n",
    "            peak_index, left, right = peak_finding(extract_c, threshold)  ## 关键函数，峰提取\n",
    "            if len(peak_index) != 0:  ### 判断是否找到峰\n",
    "                df3 = df2[df2.columns[peak_index]]\n",
    "                rt = np.round(RT[peak_index], 2)\n",
    "                intensity = np.round(np.array(df3.max().values), 0)\n",
    "                mz = np.round(np.array(df3.idxmax().values), 4)\n",
    "                name = 'peak' + str(num_p)\n",
    "                locals()[name] = np.array([rt, mz, intensity]).T\n",
    "                num_p += 1\n",
    "        p = round(num / l * 100, 1)\n",
    "        print(f' \\r finding peaks...{p}%                   ', end='')  ### 可以切换成百分比\n",
    "        num += 1\n",
    "    data = []\n",
    "    for i in range(num_p - 1):\n",
    "        name = 'peak' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    peak_info = np.concatenate(data)\n",
    "    peak_info_df = pd.DataFrame(data=peak_info, columns=['rt', 'mz', 'intensity'])\n",
    "    return peak_info_df\n",
    "\n",
    "\n",
    "def ms_locator(df1, ppm=50):\n",
    "    '''\n",
    "    For pick picking, selecting a series of mass locators for 50-1000.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param ppm: the mass difference between two locators\n",
    "    :return: mass locators\n",
    "    '''\n",
    "    @jit(nopython=True)\n",
    "    def find_locator(list1, error):\n",
    "        locators = []\n",
    "        locator = list1[0]\n",
    "        for i in range(len(list1)):\n",
    "            if list1[i] > locator:\n",
    "                locators.append(i)\n",
    "                locator *= (1 + error * 1e-6)\n",
    "        return locators\n",
    "\n",
    "    ms_list = list(df1.index)\n",
    "    typed_a = List()\n",
    "    [typed_a.append(x) for x in ms_list]\n",
    "    locators = find_locator(typed_a, ppm)\n",
    "    return locators\n",
    "\n",
    "\n",
    "def sep_scans(path, company):\n",
    "    '''\n",
    "    To separate scan for MS1, MS2 and lockspray. Only supported for Waters .raw and Agilent .d file\n",
    "    :param path: The path for mzML files\n",
    "    :return: ms1, ms2 and lockspray\n",
    "    '''\n",
    "    if company == 'Waters':\n",
    "        a = time.time()\n",
    "        print('\\r Reading files...             ', end=\"\")\n",
    "        run = pymzml.run.Reader(path)\n",
    "        ms1, ms2 = [], []\n",
    "        lockspray = []\n",
    "        for scan in run:\n",
    "            if scan.id_dict['function'] == 1:\n",
    "                ms1.append(scan)\n",
    "            if scan.id_dict['function'] == 2:\n",
    "                ms2.append(scan)\n",
    "            if scan.id_dict['function'] == 3:\n",
    "                lockspray.append(scan)\n",
    "        b = time.time()\n",
    "        time1 = round(b - a, 2)\n",
    "        print(f'\\r Reading files finished! Total time: {time1} s           ', end='')\n",
    "        return ms1, ms2, lockspray\n",
    "    elif company == 'Agilent':\n",
    "        a = time.time()\n",
    "        print('\\r Reading files...             ', end=\"\")\n",
    "        run = pymzml.run.Reader(path)\n",
    "        ms1, ms2 = [], []\n",
    "        for i, scan in enumerate(run):\n",
    "            if scan.ms_level == 1:\n",
    "                ms1.append(scan)\n",
    "            else:\n",
    "                ms2.append(scan)\n",
    "        b = time.time()\n",
    "        time1 = round(b - a, 2)\n",
    "        print(f'\\r Reading files finished! Total time: {time1} s           ', end='')\n",
    "        \n",
    "        return ms1, ms2\n",
    "\n",
    "\n",
    "def peak_finding(eic, threshold=15):\n",
    "    '''\n",
    "    finding peaks in a single extracted chromatogram,and return peak index, left valley index, right valley index.\n",
    "    :param eic: extracted ion chromatogram data; e.g., [1,2,3,2,3,1...]\n",
    "    :param threshold: define the noise level for a peak, 6 is recommend\n",
    "    :return:peak index, left valley index, right valley index.\n",
    "    '''\n",
    "    peaks, _ = scipy.signal.find_peaks(eic, width=2)\n",
    "    prominence = scipy.signal.peak_prominences(eic, peaks)\n",
    "    peak_prominence = prominence[0]\n",
    "    left = prominence[1]\n",
    "    right = prominence[2]\n",
    "    ### peak_picking condition 1: value of peak_prominence must be higher than\n",
    "    len_pro = len(peak_prominence)\n",
    "    if len(peak_prominence) == 0:\n",
    "        peak_index, left, right = np.array([]), np.array([]), np.array([])\n",
    "    else:\n",
    "        median_1 = np.median(peak_prominence)  ### 获得中位数的值\n",
    "        index_pos2 = where(prominence[0] > threshold * median_1)[0]\n",
    "        peak_index = peaks[index_pos2]\n",
    "        left = left[index_pos2]\n",
    "        right = right[index_pos2]\n",
    "    return peak_index, left, right\n",
    "\n",
    "\n",
    "\n",
    "def extract(df1, mz, error=50):\n",
    "    '''\n",
    "    Extracting chromatogram based on mz and error.\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param mz: Targeted mass for extraction.\n",
    "    :param error: mass error for extraction\n",
    "    :return: rt,eic\n",
    "    '''\n",
    "    low = mz * (1 - error * 1e-6)\n",
    "    high = mz * (1 + error * 1e-6)\n",
    "    low_index = argmin(abs(df1.index.values - low))\n",
    "    high_index = argmin(abs(df1.index.values - high))\n",
    "    df2 = df1.iloc[low_index:high_index]\n",
    "    rt = df1.columns.values\n",
    "    if len(np.array(df2)) == 0:\n",
    "        intensity = np.zeros(len(df1.columns))\n",
    "    else:\n",
    "        intensity = np.array(df2).T.sum(axis=1)\n",
    "    return rt, intensity  ### 只返回RT和EIC\n",
    "\n",
    "\n",
    "\n",
    "def gen_df_to_centroid(ms1, ms_round=4):\n",
    "    '''\n",
    "    Convert mzml data to a dataframe in centroid mode.\n",
    "    :param ms1: ms scan list generated by the function of sep_scans(), or directed from pymzml.run.Reader(path).\n",
    "    :return: A Dataframe\n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    l = len(ms1)\n",
    "    num = 0\n",
    "    print('\\r Generating dataframe...             ', end=\"\")\n",
    "    ###将所有的数据转换成centroid格式，并将每个scan存在一个独立的变量scan(n)中\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        peaks, _ = scipy.signal.find_peaks(ms1[i].i.copy())\n",
    "        locals()[name] = pd.Series(data=ms1[i].i[peaks], index=ms1[i].mz[peaks].round(ms_round),\n",
    "                                   name=round(ms1[i].scan_time[0], 3))\n",
    "        t2 = time.time()\n",
    "        total_t = round(t2 - t1, 2)\n",
    "        p = round(num / l * 100, 2)\n",
    "        print(f'\\r Reading each scans：{total_t} s, {num}/{l}, {p} %     ', end=\"\")\n",
    "        num += 1\n",
    "    ### 将所有的变量汇总到一个列表中\n",
    "    data = []\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    t3 = time.time()\n",
    "    ## 开始级联所有数据\n",
    "    print('\\r Concatenating all the data...                   ', end=\"\")\n",
    "    df1 = pd.concat(data, axis=1)\n",
    "    df2 = df1.fillna(0)\n",
    "    t4 = time.time()\n",
    "    t = round(t4 - t1, 2)\n",
    "    print(f'\\r Concat finished, Consumed time: {t} s            ', end='')\n",
    "    return df2\n",
    "\n",
    "\n",
    "def gen_df_raw(ms1, ms_round=4):\n",
    "    '''\n",
    "    Convert mzml data to a dataframe in profile mode.\n",
    "    :param ms1: ms scan list generated by the function of sep_scans(), or directed from pymzml.run.Reader(path).\n",
    "    :return: A Dataframe\n",
    "    '''\n",
    "    t1 = time.time()\n",
    "    l = len(ms1)\n",
    "    num = 0\n",
    "    print('\\r Generating dataframe...             ', end=\"\")\n",
    "    ###将每个scan存在一个独立的变量scan(n)中\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        locals()[name] = pd.Series(data=ms1[i].i, index=ms1[i].mz.round(ms_round), name=round(ms1[i].scan_time[0], 3))\n",
    "        t2 = time.time()\n",
    "        total_t = round(t2 - t1, 2)\n",
    "        p = round(num / l * 100, 2)\n",
    "        print(f'\\r Reading each scans：{total_t} s, {num}/{l}, {p} %', end=\"\")\n",
    "        num += 1\n",
    "    ### 将所有的变量汇总到一个列表中\n",
    "    data = []\n",
    "    for i in range(l):\n",
    "        name = 'scan' + str(i)\n",
    "        data.append(locals()[name])\n",
    "    t3 = time.time()\n",
    "    ## 开始级联所有数据\n",
    "    print('\\r Concatenating all the data...                             ', end=\"\")\n",
    "    df1 = pd.concat(data, axis=1)\n",
    "    df2 = df1.fillna(0)\n",
    "    t4 = time.time()\n",
    "    t = round(t4 - t1, 2)\n",
    "    print(f'\\r Concat finished, Consumed time: {t} s                     ', end='')\n",
    "    return df2\n",
    "\n",
    "\n",
    "def B_spline(x, y):\n",
    "    '''\n",
    "    Generating more data points for a mass peak using beta-spline based on x,y\n",
    "    :param x: mass coordinates\n",
    "    :param y: intensity\n",
    "    :return: new mass coordinates, new intensity\n",
    "    '''\n",
    "    t, c, k = interpolate.splrep(x, y, s=0, k=4)\n",
    "    N = 300\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    new_x = np.linspace(xmin, xmax, N)\n",
    "    spline = interpolate.BSpline(t, c, k, extrapolate=False)\n",
    "    return new_x, spline(new_x)\n",
    "\n",
    "\n",
    "def cal_bg(data):\n",
    "    '''\n",
    "    :param data: data need to calculate the background\n",
    "    :return: background value\n",
    "    '''\n",
    "    if len(data) > 5:\n",
    "        Median = median(data)\n",
    "        Max_value = max(data)\n",
    "        STD = std(data)\n",
    "        Mean = mean(data)\n",
    "        if Median == 0:\n",
    "            bg = Mean + STD\n",
    "        elif Mean <= Median * 5:\n",
    "            bg = Max_value\n",
    "        elif Mean > Median * 5:\n",
    "            bg = Median\n",
    "    else:\n",
    "        bg = 1000000\n",
    "    return bg + 1\n",
    "\n",
    "\n",
    "def peak_checking_plot(df1, mz, rt1, Type='profile', path=None):\n",
    "    '''\n",
    "    Evaluating/visulizing the extracted mz\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param mz: Targetd mass for extraction\n",
    "    :param rt1: expected rt for peaks\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ### 检查色谱图ax\n",
    "    ax = fig.add_subplot(121)\n",
    "    rt, eic = extract(df1, mz, 50)\n",
    "    rt2 = rt[where((rt > rt1 - 2) & (rt < rt1 + 2))]\n",
    "    eic2 = eic[where((rt > rt1 - 2) & (rt < rt1 + 2))]\n",
    "    ax.plot(rt2, eic2)\n",
    "    ax.set_xlabel('Retention Time(min)', fontsize=12)\n",
    "    ax.set_ylabel('Intensity', fontsize=12)\n",
    "    peak_index = np.argmin(abs(rt - rt1))\n",
    "    peak_height = max(eic[peak_index - 2:peak_index + 2])\n",
    "    ax.scatter(rt1, peak_height * 1.05, c='r', marker='*', s=50)\n",
    "    ##计算背景\n",
    "    bg_left, bg_right = cal_bg(eic2[:50]), cal_bg(eic2[-50:])\n",
    "    rt3 = rt2[:50]\n",
    "    rt4 = rt2[-50:]\n",
    "    bg1 = zeros(50) + bg_left\n",
    "    bg2 = zeros(50) + bg_right\n",
    "    ax.plot(rt3, bg1)\n",
    "    ax.plot(rt4, bg2)\n",
    "    SN1 = round(peak_height / bg_left, 1)\n",
    "    SN2 = round(peak_height / bg_right, 1)\n",
    "    ax.set_title(f'SN_left:{SN1},         SN_right:{SN2}')\n",
    "    ax.set_ylim(top=peak_height * 1.1, bottom=-peak_height * 0.05)\n",
    "\n",
    "    ### 检查质谱图ax1\n",
    "    ax1 = fig.add_subplot(122)\n",
    "    width = 0.02\n",
    "    spec = spec_at_rt(df1,rt1)  ## 提取到特定时间点的质谱图\n",
    "    new_spec = target_spec(spec, mz, width=0.04)\n",
    "    \n",
    "    if Type == 'profile':\n",
    "        mz_obs, error1, mz_opt, error2, resolution = evaluate_ms(new_spec, mz)\n",
    "        ax1.plot(new_spec)\n",
    "        ax1.bar(mz, max(new_spec.values), color='r', width=0.0005)\n",
    "        ax1.bar(mz_opt,max(new_spec.values), color='g', width=0.0005)\n",
    "        ax1.text(min(new_spec.index.values)+0.005, max(new_spec.values)*0.8, \n",
    "             f'mz_obs: {mz_obs},{error1} \\n mz_opt:{mz_opt}, {error2}')\n",
    "    else:\n",
    "        ax1.bar(mz1, max(new_spec.values), width=0.0002)\n",
    "\n",
    "    \n",
    "    ax1.set_title(f'mz_exp: {mz}')\n",
    "    ax1.set_xlabel('m/z', fontsize=12)\n",
    "    ax1.set_ylabel('Intensity', fontsize=12)\n",
    "    ax1.set_xlim(mz - 0.04, mz + 0.04)\n",
    "\n",
    "    if path == None:\n",
    "        pass\n",
    "    else:\n",
    "        plt.savefig(path, dpi=1000)\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def peak_alignment(files_excel, rt_error=0.1, mz_error=0.015):\n",
    "    '''\n",
    "    Generating peaks information with reference mz/rt pair\n",
    "    :param files_excel: files for excels of peak picking and peak checking;\n",
    "    :param rt_error: rt error for merge\n",
    "    :param mz_error: mz error for merge\n",
    "    :return: Export to excel files\n",
    "    '''\n",
    "    print('\\r Generating peak reference...        ', end='')\n",
    "    peak_ref = gen_ref(files_excel, rt_error=rt_error, mz_error=mz_error)\n",
    "    j = 1\n",
    "    for file in files_excel:\n",
    "        print(f'\\r for {j} files, generating alignment results...', end='')\n",
    "        peak_p = pd.read_excel(file, index_col='Unnamed: 0').loc[:, ['rt', 'mz']].values\n",
    "        peak_df = pd.read_excel(file, index_col='Unnamed: 0')\n",
    "        new_all_index = []\n",
    "        for i in range(len(peak_p)):\n",
    "            rt1, mz1 = peak_p[i]\n",
    "            index = np.where((peak_ref[:, 0] <= rt1 + rt_error) & (peak_ref[:, 0] >= rt1 - rt_error)\n",
    "                             & (peak_ref[:, 1] <= mz1 + mz_error) & (peak_ref[:, 1] >= mz1 - mz_error))\n",
    "            new_index = str(peak_ref[index][0][0]) + '_' + str(peak_ref[index][0][1])\n",
    "            new_all_index.append(new_index)\n",
    "        peak_df['new_index'] = new_all_index\n",
    "        peak_df = peak_df.set_index('new_index')\n",
    "        peak_df.to_excel(file.replace('.xlsx', '_alignment.xlsx'))\n",
    "        j += 1\n",
    "\n",
    "\n",
    "def database_evaluation(database, i, df1, df2, path=None):\n",
    "    '''\n",
    "    :param database: excel file containing compounds' information\n",
    "    :param i:  the index for a row in excel\n",
    "    :param df1: ms1 dataframe\n",
    "    :param df2: ms2 dataframe\n",
    "    :return:\n",
    "    '''\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "    formula = database.loc[i, 'Molecular Formula']\n",
    "    rt_exp = database.loc[i, 'rt']\n",
    "    mz_exp = round(database.loc[i, 'Positive'], 4)\n",
    "    f = Formula(formula)\n",
    "    a = f.spectrum()\n",
    "    mz_iso, i_iso = np.array([a for a in a.values()]).T\n",
    "    i_iso = i_iso / i_iso[0] * 100\n",
    "    mz_iso += 1.00727647  ## 加个H\n",
    "    frag_exp = float(database.loc[i, 'Fragment1'])\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "\n",
    "    ### 1. 对MS1进行色谱峰提取\n",
    "    ax1 = fig.add_subplot(231)\n",
    "    ax1.set_title('MS1 extracted chromatogram', fontsize=10)\n",
    "    rt1, eic1 = extract(df1, mz_exp, 50)\n",
    "    ax1.plot(rt1, eic1)\n",
    "\n",
    "    ax1.set_ylabel('Intensity', fontsize=12)\n",
    "    peak_index = np.argmin(abs(rt1 - rt_exp))\n",
    "    rt_obs = round(rt1[peak_index - 20:peak_index + 20][np.argmax(eic1[peak_index - 20:peak_index + 20])], 2)  ###找到峰的位置\n",
    "\n",
    "    peak_height = max(eic1[peak_index - 2:peak_index + 2])\n",
    "    ax1.scatter(rt_exp, peak_height * 1.05, c='r', marker='*', s=50)\n",
    "    left1 = rt_exp - 1\n",
    "    right1 = rt_exp + 1\n",
    "\n",
    "    ax1.text(rt_obs + 0.05, peak_height * 0.7, f'RT_obs:{rt_obs} min \\n RT_exp:{rt_exp} min')\n",
    "\n",
    "    ax1.set_xlim(left=left1, right=right1)\n",
    "    ax1.set_ylim(top=peak_height * 1.1, bottom=-peak_height * 0.05)\n",
    "\n",
    "    ### 2. MS1质谱图质量评估\n",
    "    ax2 = fig.add_subplot(232)\n",
    "    ax2.set_title('Isotope check/MS1', fontsize=10)\n",
    "    ind = np.argmin(abs(df1.columns.values - rt_exp))\n",
    "    mz2, i2 = df1.iloc[:, ind].index, df1.iloc[:, ind].values\n",
    "\n",
    "    peaks, _ = scipy.signal.find_peaks(i2)\n",
    "    index1 = peaks[np.argmin(abs(mz2[peaks] - (mz_exp - 1)))]\n",
    "    index2 = peaks[np.argmin(abs(mz2[peaks] - (mz_exp + 5)))]\n",
    "    index_obs = peaks[np.argmin(abs(mz2[peaks] - mz_exp))]\n",
    "    mz_obs = mz2[index_obs]\n",
    "    height_obs = i2[index_obs]  ## 一定要放在之前\n",
    "    mz2, i2 = mz2[index1:index2], i2[index1:index2]\n",
    "\n",
    "    ax2.plot(mz2, i2)\n",
    "    ax2.bar(mz_iso, -(height_obs / 100) * i_iso, width=0.03, color=['r'], zorder=2)\n",
    "    ax2.text(mz_iso[0], -(height_obs / 100) * i_iso[0], round(mz_iso[0], 4))\n",
    "    ax2.text(mz_iso[1], -(height_obs / 100) * i_iso[1], round(mz_iso[1], 4))\n",
    "    ax2.text(mz_iso[2], -(height_obs / 100) * i_iso[2], round(mz_iso[2], 4))\n",
    "\n",
    "    ### 3. 检查MS1质谱准确性\n",
    "    ax3 = fig.add_subplot(233)\n",
    "    ax3.set_title('Accurate mz check/MS1', fontsize=10)\n",
    "    index3 = argmin(abs(mz2 - mz_exp)) - 12\n",
    "    index4 = argmin(abs(mz2 - mz_exp)) + 12\n",
    "    mz3, i3 = mz2[index3:index4], i2[index3:index4]\n",
    "    mz3_opt, i3_opt = B_spline(mz3, i3)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax3.plot(mz3, i3, marker='o')  ### 原始数据\n",
    "    ax3.plot(mz3_opt, i3_opt, c='r', lw=0.5)  ### 优化数据\n",
    "    ax3.bar(mz_exp, height_obs, width=0.001, color=['g', 'b'])  ## 理论质量\n",
    "    obs_error = round((mz_obs - mz_exp) / mz_exp * 1000000, 1)\n",
    "    opt_error = round((mz_opt - mz_exp) / mz_exp * 1000000, 1)\n",
    "    ax3.text(mz_obs - 0.05, height_obs * 0.8,\n",
    "             f'  obs: {round(mz_obs, 4)} \\n  obs_err: {obs_error} \\n  opt: {round(mz_opt, 4)} \\n  opt_err: {opt_error}')\n",
    "    ax3.text(mz_obs + 0.01, height_obs * 1, f'exp: {mz_exp}')\n",
    "\n",
    "    ## 4. 检查DFI 提取色谱图\n",
    "    if np.isnan(frag_exp):\n",
    "        print(f'frag_exp is not given, current frag_exp: {frag_exp}')\n",
    "    else:\n",
    "        ax4 = fig.add_subplot(234)\n",
    "        ax4.set_title('mass spectrum/DFI', fontsize=10)\n",
    "        ax4.set_xlabel('Retention Time(min)', fontsize=12)\n",
    "\n",
    "        rt4, eic4 = extract(df2, frag_exp, 50)  ##提取改成df2\n",
    "        ax4.plot(rt4, eic4)\n",
    "        ax4.set_ylabel('Intensity', fontsize=12)\n",
    "        peak_index = np.argmin(abs(rt4 - rt_exp))\n",
    "        peak_height4 = max(eic4[peak_index - 2:peak_index + 2])\n",
    "        ax4.scatter(rt_exp, peak_height4 * 1.05, c='r', marker='*', s=50)\n",
    "        left4 = rt_exp - 1\n",
    "        right4 = rt_exp + 1\n",
    "        ax4.set_xlim(left=left4, right=right4)\n",
    "        ax4.text(rt_exp + 0.05, peak_height4 * 0.7, f' RT_exp:{rt_exp} min')\n",
    "        ax4.set_ylim(top=peak_height4 * 1.1, bottom=-peak_height4 * 0.05)\n",
    "\n",
    "        ### 5. 检查DFI同位素峰\n",
    "        ax5 = fig.add_subplot(235)\n",
    "        ax5.set_title('DFI check', fontsize=10)\n",
    "        ax5.set_xlabel('m/z', fontsize=12)\n",
    "        index5 = np.argmin(abs(df2.columns.values - rt_obs))\n",
    "        mz5, i5 = df2.iloc[:, index5].index.values, df2.iloc[:, index5].values\n",
    "        index5_1 = np.argmin(abs(mz5 - frag_exp))\n",
    "        height_obs = i5[index5_1]\n",
    "        index5_2 = np.argmin(abs(mz5 - (frag_exp - 1)))\n",
    "        index5_3 = np.argmin(abs(mz5 - (frag_exp + 4)))\n",
    "        mz5, i5 = mz5[index5_2:index5_3], i5[index5_2:index5_3]\n",
    "        ax5.plot(mz5, i5)\n",
    "        frag_iso = mz_iso - (mz_iso - frag_exp)[0]\n",
    "        ax5.bar(frag_iso + 0.1, (height_obs / 100) * i_iso, width=0.03, color=['m'], zorder=2)\n",
    "        ax5.text(frag_iso[0] + 0.1, (height_obs / 100) * i_iso[0], round(frag_iso[0], 4))\n",
    "        ax5.text(frag_iso[1] + 0.1, (height_obs / 100) * i_iso[1], round(frag_iso[1], 4))\n",
    "        ax5.text(frag_iso[2] + 0.1, (height_obs / 100) * i_iso[2], round(frag_iso[2], 4))\n",
    "\n",
    "        ### 6. 检查DFI质谱峰\n",
    "        ax6 = fig.add_subplot(236)\n",
    "        ax6.set_title('Accurate DFI check', fontsize=10)\n",
    "        ax6.set_xlabel('m/z', fontsize=12)\n",
    "        index6_1 = argmin(abs(mz5 - frag_exp)) - 12\n",
    "        index6_2 = argmin(abs(mz5 - frag_exp)) + 12\n",
    "        mz6, i6 = mz5[index6_1:index6_2], i5[index6_1:index6_2]\n",
    "        frag_obs = round(mz6[np.argmax(i6)], 4)\n",
    "        mz6_opt, i6_opt = B_spline(mz6, i6)\n",
    "        frag_opt = mz6_opt[np.argmax(i6_opt)]\n",
    "\n",
    "        ax6.plot(mz6, i6, marker='o')  ### 原始数据\n",
    "        ax6.plot(mz6_opt, i6_opt, c='r', lw=0.5)  ### 优化数据\n",
    "        ax6.bar(frag_exp, height_obs, width=0.001, color=['g'])\n",
    "\n",
    "        obs_error = round((frag_obs - frag_exp) / mz_exp * 1000000, 1)\n",
    "        opt_error = round((frag_opt - frag_exp) / mz_exp * 1000000, 1)\n",
    "        ax6.text(frag_obs - 0.04, height_obs * 0.8,\n",
    "                 f'  obs: {round(frag_obs, 4)} \\n  obs_err: {obs_error} \\n  opt: {round(frag_opt, 4)} \\n  opt_err: {opt_error}')\n",
    "        ax6.text(frag_obs + 0.01, height_obs * 1, f'exp: {frag_exp}')\n",
    "        \n",
    "    if path ==None:\n",
    "        pass\n",
    "    else:   \n",
    "        fig.savefig(path, dpi=1000)\n",
    "        plt.close(fig)\n",
    "\n",
    "def peak_checking(peak_df, df1, error=50,\n",
    "                  i_threshold=500, SN_threshold=5):\n",
    "    '''\n",
    "    Processing extracted peaks, remove those false positives.\n",
    "    :param peak_df: Extracted peaks generated by the function of peak_picking\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param error: For the function of extract(df,mz, error)\n",
    "    :param i_threshold: filter peaks with intensity<i_threshold\n",
    "    :param SN_threshold: filter peaks with sn<SN_threshold\n",
    "    :return:\n",
    "    '''\n",
    "    final_result = pd.DataFrame()\n",
    "    n = 0\n",
    "    peak_num = len(peak_df['rt'])\n",
    "    SN_all_left, SN_all_right, area_all, cor_all_mz, cor_all_i = [], [], [], [], []\n",
    "    for i in range(peak_num):\n",
    "        mz = peak_df.iloc[i]['mz']\n",
    "        rt = peak_df.iloc[i]['rt']\n",
    "        ### 第一步：处理色谱峰\n",
    "        rt_e, eic_e = extract(df1, mz, error=error)\n",
    "        peak_index = np.argmin(abs(rt_e - rt))  ## 找到特定时间点的索引\n",
    "        rt_left = rt - 0.2\n",
    "        rt_right = rt + 0.2\n",
    "        peak_index_left = np.argmin(abs(rt_e - rt_left))\n",
    "        peak_index_right = np.argmin(abs(rt_e - rt_right))\n",
    "        mz_all, intensity_t = df1.iloc[:, peak_index].index.values, df1.iloc[:, peak_index].values  ## 提取特定时间的质谱峰\n",
    "        try:\n",
    "            peak_height = max(eic_e[peak_index - 2:peak_index + 2])\n",
    "            other_peak = max(eic_e[peak_index - 5:peak_index + 5])\n",
    "        except:\n",
    "            peak_height = 1\n",
    "            other_peak = 3\n",
    "        rt_t, eic_t = rt_e[peak_index_left:peak_index_right], eic_e[peak_index_left:peak_index_right]\n",
    "        try:\n",
    "            area = scipy.integrate.simps(eic_e[peak_index - 40:peak_index + 40])\n",
    "        except:\n",
    "            area = scipy.integrate.simps(eic_e)\n",
    "        if other_peak - peak_height > 1:\n",
    "            bg_left, bg_right = 10000000, 10000000\n",
    "        else:\n",
    "            bg_left = cal_bg(eic_t[:50])\n",
    "            bg_right = cal_bg(eic_t[-50:])\n",
    "\n",
    "        SN_left = round(peak_height / bg_left, 1)\n",
    "        SN_right = round(peak_height / bg_right, 1)\n",
    "        SN_all_left.append(SN_left)\n",
    "        SN_all_right.append(SN_right)\n",
    "        area_all.append(area)\n",
    "\n",
    "        ### 第二步：处理质谱峰\n",
    "        mz_l, mz_h = mz - 0.005, mz + 0.005\n",
    "        mz_all, intensity_t = df1.iloc[:, peak_index].index.values, df1.iloc[:, peak_index].values\n",
    "        mz_index = argmin(abs(mz_all - mz))\n",
    "        mz_width = 20\n",
    "        mz_, i_ = mz_all[mz_index - mz_width:mz_index + mz_width], intensity_t[mz_index - mz_width:mz_index + mz_width]\n",
    "        peaks, _ = scipy.signal.find_peaks(i_)\n",
    "        peak_mz, peak_i = mz_[peaks], i_[peaks]\n",
    "        peak_mz, peak_i = peak_mz[(peak_mz >= mz_l) & (peak_mz <= mz_h)], peak_i[(peak_mz >= mz_l) & (peak_mz <= mz_h)]\n",
    "        if len(peak_mz) == 0:\n",
    "            cor_mz, cor_i = 0, 1\n",
    "        else:\n",
    "            cor_mz = round(peak_mz[argmax(peak_i)], 4)\n",
    "            cor_i = round(peak_i[argmax(peak_i)], 0)\n",
    "        cor_all_mz.append(cor_mz)\n",
    "        cor_all_i.append(cor_i)\n",
    "        n += 1\n",
    "        print(f'\\r Processing peaks...{n}/{peak_num}                        ', end='')\n",
    "    final_result['SN_left'] = SN_all_left\n",
    "    final_result['SN_right'] = SN_all_right\n",
    "    final_result['area'] = list(map(int, area_all))\n",
    "    final_result['mz'] = cor_all_mz\n",
    "    final_result['intensity'] = cor_all_i\n",
    "    final_result['rt'] = peak_df['rt']\n",
    "    ### 筛选条件，峰强度> i_threshold; 左边和右边SN至少一个大于SN_threshold\n",
    "    final_result = final_result[(final_result['intensity'] > i_threshold) &\n",
    "                                ((final_result['SN_left'] > SN_threshold) | (final_result['SN_right'] > SN_threshold))]\n",
    "    final_result = final_result.loc[:, ['rt', 'mz', 'intensity', 'SN_left', 'SN_right', 'area']].sort_values(\n",
    "        by='intensity').reset_index(drop=True)\n",
    "\n",
    "    return final_result\n",
    "\n",
    "\n",
    "def spec_at_rt(df1, rt):\n",
    "    '''\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param rt:  rentention time for certain ms spec\n",
    "    :return: ms spec\n",
    "    '''\n",
    "    index = argmin(abs(df1.columns.values - rt))\n",
    "    spec = df1.iloc[:, index]\n",
    "    return spec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def concat_alignment(files_excel):\n",
    "    '''\n",
    "    Concatenate all data and return\n",
    "    :param files_excel: excel files\n",
    "    :param mode: selected 'area' or 'intensity' for each sample\n",
    "    :return: dataframe\n",
    "    '''\n",
    "    align = []\n",
    "    data_to_concat = []\n",
    "    for i in range(len(files_excel)):\n",
    "        if 'alignment' in files_excel[i]:\n",
    "            align.append(files_excel[i])\n",
    "    for i in range(len(align)):\n",
    "        name = 'data' + str(i)\n",
    "        locals()[name] = pd.read_excel(align[i], index_col='Unnamed: 0')\n",
    "        data_to_concat.append(locals()[name])\n",
    "    final_data = pd.concat(data_to_concat, axis=1)\n",
    "    return final_data\n",
    "\n",
    "\n",
    "\n",
    "def formula_to_distribution(formula, adducts='+H', num=3):\n",
    "    '''\n",
    "    :param formula: molecular formula, e.g., ‘C13H13N3’\n",
    "    :param adducts: ion adducts, '+H', '-H'\n",
    "    :return: mz_iso, i_iso (np.array)\n",
    "    '''\n",
    "    f = Formula(formula)\n",
    "    a = f.spectrum()\n",
    "    mz_iso, i_iso = np.array([a for a in a.values()]).T\n",
    "    i_iso = i_iso / i_iso[0] * 100\n",
    "    if adducts == '+H':\n",
    "        mz_iso += 1.00727647\n",
    "    elif adducts == '-H':\n",
    "        mz_iso -= 1.00727647\n",
    "    mz_iso = mz_iso.round(4)\n",
    "    i_iso = i_iso.round(1)\n",
    "    return mz_iso[:num], i_iso[:num]\n",
    "\n",
    "\n",
    "def add_ms_values(new_spec):\n",
    "    '''\n",
    "    :param new_spec: the spectrum (pandas.Series)\n",
    "    :return:  peak_mz,peak_i\n",
    "    '''\n",
    "    peaks, _ = scipy.signal.find_peaks(new_spec.values)\n",
    "    peak3 = new_spec.iloc[peaks].sort_values().iloc[-3:]\n",
    "    peak_mz = peak3.index.values\n",
    "    peak_i = peak3.values\n",
    "    return peak_mz, peak_i\n",
    "\n",
    "\n",
    "def multi_process(file, company):\n",
    "    ms1, *_ = sep_scans(file, company)\n",
    "    df1 = gen_df_raw(ms1)\n",
    "    peak_all = peak_picking(df1)\n",
    "    peak_selected = peak_checking(peak_all, df1)\n",
    "    peak_selected.to_excel(file.replace('.mzML', '.xlsx'))\n",
    "\n",
    "\n",
    "def KMD_cal(mz_set, group='Br/H'):\n",
    "    if '/' in group:\n",
    "        g1, g2 = group.split('/')\n",
    "        f1, f2 = Formula(g1), Formula(g2)\n",
    "        f1, f2 = f1.spectrum(), f2.spectrum()\n",
    "        f1_value, f2_value = [x for x in f1.values()][0][0], [x for x in f2.values()][0][0]\n",
    "        values = [abs(f1_value - f2_value), round(abs(f1_value - f2_value), 0)]\n",
    "        KM = mz_set * (max(values) / min(values))\n",
    "        KMD_set = KM - np.floor(KM)\n",
    "\n",
    "        print(f1_value, f2_value)\n",
    "        print(min(values), max(values))\n",
    "        print(values)\n",
    "    else:\n",
    "        g1 = Formula(group)\n",
    "        f1 = g1.spectrum()\n",
    "        f1_value = [x for x in f1.values()][0][0]\n",
    "        KM = mz_set * (int(f1_value) / f1_value)\n",
    "        KMD_set = KM - np.floor(mz_set)\n",
    "    return KMD_set\n",
    "\n",
    "\n",
    "def sep_result(result, replicate=4, batch=5):\n",
    "    a = 0\n",
    "    sep_result = []\n",
    "    for i in range(batch):\n",
    "        name = 'b' + str(i)\n",
    "        sep_result.append(result[result.columns[a:a + replicate]])\n",
    "        a += replicate\n",
    "\n",
    "    return sep_result\n",
    "\n",
    "\n",
    "\n",
    "def peak_checking_area(ref_all,df1,name='area'):\n",
    "    '''\n",
    "    Based on referece pairs, extract all peaks and integrate the peak area.\n",
    "    :param ref_all: all referece pairs (dataframe)\n",
    "    :param df1: LC-MS dataframe, genrated by the function gen_df()\n",
    "    :param name: name for area\n",
    "    :return: peak_ref (dataframe)\n",
    "    '''\n",
    "    area_all = []\n",
    "    peak_index = np.array(ref_all['rt'].map(lambda x:str(round(x,2))).str.cat(ref_all['mz'].map(lambda x:str(round(x,4))),sep = '_'))\n",
    "    num = len(ref_all)\n",
    "    for i in range(num):\n",
    "        rt,mz =ref_all.loc[i,['rt','mz']]\n",
    "        rt1,eic1 = extract(df1,mz,50)\n",
    "        rt_ind = argmin(abs(rt1-rt))\n",
    "        left = argmin(abs(rt1-(rt-0.2)))\n",
    "        right = argmin(abs(rt1-(rt+0.2)))\n",
    "        rt_t,eic_t = rt1[left:right],eic1[left:right]\n",
    "        area = round(scipy.integrate.simps(eic_t,rt_t),0)\n",
    "        area_all.append(area)\n",
    "        print(f'\\r {i}/{num}', end = '')\n",
    "    peak_ref = pd.DataFrame(area_all,index = peak_index,columns = [name])\n",
    "    return peak_ref\n",
    "\n",
    "\n",
    "def JsonToExcel(path):\n",
    "    with open(path,'r',encoding='utf8')as fp:\n",
    "        json_data = json.load(fp)\n",
    "    Inchikey,precursor,frag,formula,smiles = [],[],[],[],[]\n",
    "    num = len(json_data)\n",
    "    for i in range(num):\n",
    "        try:\n",
    "            cmp_info = json_data[i]['compound'][0]['metaData']\n",
    "            Inchikey.append([x['value'] for x in cmp_info if x['name']=='InChIKey'][0])\n",
    "            formula.append([x['value'] for x in cmp_info if x['name']=='molecular formula'][0])\n",
    "            precursor.append([x['value'] for x in cmp_info if x['name']=='total exact mass'][0])\n",
    "            smiles.append([x['value'] for x in cmp_info if x['name']=='SMILES'][0])\n",
    "        except:\n",
    "            Inchikey.append(None)\n",
    "            formula.append(None)\n",
    "            precursor.append(None)\n",
    "            smiles.append(None)\n",
    "        frag.append(r'{' + json_data[i]['spectrum'].replace(' ',',') + r'}')    \n",
    "        print(f'\\r {round(i/num*100,2)}%',end='')\n",
    "    database = pd.DataFrame(np.array([Inchikey,precursor,frag,formula,smiles]).T,\n",
    "                            columns = ['Inchikey','Precursor','Frag','Formula','Smiles'])\n",
    "    return database\n",
    "\n",
    "\n",
    "def evaluate_ms(new_spec,mz_exp):\n",
    "    peaks,_ = scipy.signal.find_peaks(new_spec.values)\n",
    "    mz_obs = new_spec.index.values[peaks][argmin(abs(new_spec.index.values[peaks]-mz_exp))]\n",
    "    x, y = B_spline(new_spec.index.values, new_spec.values)\n",
    "    peaks,_ = scipy.signal.find_peaks(y)\n",
    "    max_index = peaks[argmin(abs(x[peaks]-mz_exp))]\n",
    "    half_height = y[peak_index]/2\n",
    "    mz_left = x[:max_index][argmin(abs(y[:max_index] - half_height))]\n",
    "    mz_right = x[max_index:][argmin(abs(y[max_index:] - half_height))]\n",
    "    resolution = int(mz_obs / (mz_right - mz_left))\n",
    "    mz_opt = round(mz_left+(mz_right - mz_left)/2, 4)\n",
    "    mz_opt_ref = round(x[max_index],4)\n",
    "    \n",
    "    if abs(mz_opt-mz_opt_ref)/mz_exp*1000000 <10:\n",
    "        final_mz_opt = mz_opt\n",
    "    else:\n",
    "        final_mz_opt = mz_opt_ref\n",
    "        \n",
    "    error1 = round((mz_obs -mz_exp)/mz_exp*1000000,1)\n",
    "    error2 = round((final_mz_opt -mz_exp)/mz_exp*1000000,1)\n",
    "    return mz_obs,error1,final_mz_opt,error2,resolution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def target_spec(spec, target_mz, width=0.04):\n",
    "    '''\n",
    "    :param spec: spec generated from function spec_at_rt()\n",
    "    :param target_mz: target mz for inspection\n",
    "    :param width: width for data points\n",
    "    :return: new spec and observed mz\n",
    "    '''\n",
    "    index = argmin(abs(spec.index.values-target_mz))\n",
    "    index_left =argmin(abs(spec.index.values-(target_mz-width)))\n",
    "    index_right =argmin(abs(spec.index.values-(target_mz+width)))\n",
    "    new_spec = spec.iloc[index_left:index_right]\n",
    "    return new_spec\n",
    "\n",
    "\n",
    "def gen_ref(files_excel, mz_error=0.015, rt_error=0.1):\n",
    "    '''\n",
    "    For alignment, generating a reference mz/rt pair\n",
    "    :param files_excel: excel files path for extracted peaks\n",
    "    :return: mz/rt pair reference\n",
    "    '''\n",
    "    data = []\n",
    "    for i in range(len(files_excel)):\n",
    "        name = 'peaks' + str(i)\n",
    "        locals()[name] = pd.read_excel(files_excel[i], index_col='Unnamed: 0').loc[:, ['rt', 'mz']].values\n",
    "        data.append(locals()[name])\n",
    "        print(f'\\r Reading excel files... {i}/{len(files_excel)}                   ', end=\"\")\n",
    "    print(f'\\r Concatenating all peaks...                 ', end='')\n",
    "    pair = np.concatenate(data, axis=0)\n",
    "    peak_all_check = pair\n",
    "    peak_ref = []\n",
    "    while len(pair) > 0:\n",
    "        rt1, mz1 = pair[0]\n",
    "        index1 = np.where((pair[:, 0] <= rt1 + rt_error) & (pair[:, 0] >= rt1 - rt_error)\n",
    "                          & (pair[:, 1] <= mz1 + mz_error) & (pair[:, 1] >= mz1 - mz_error))\n",
    "        peak = np.mean(pair[index1], axis=0).tolist()\n",
    "        peak = [round(peak[0],2),round(peak[1],4)]\n",
    "        pair = np.delete(pair, index1, axis=0)\n",
    "        peak_ref.append(peak)\n",
    "        print(f'\\r  {len(pair)}                        ', end='')\n",
    "\n",
    "    peak_ref2 = np.array(peak_ref)\n",
    "    \n",
    "    ### 检查是否有漏的\n",
    "    peak_lost = []\n",
    "    for peak in peak_all_check:\n",
    "        rt1, mz1 = peak\n",
    "        check = np.where((peak_ref2[:, 0] <= rt1 + rt_error) & (peak_ref2[:, 0] >= rt1 - rt_error)\n",
    "                         & (peak_ref2[:, 1] <= mz1 + mz_error) & (peak_ref2[:, 1] >= mz1 - mz_error))\n",
    "        if len(check[0]) == 0:\n",
    "            peak_lost.append([rt1, mz1])\n",
    "    peak_lost=np.array(peak_lost)\n",
    "    while len(peak_lost) > 0:\n",
    "        rt1, mz1 = peak_lost[0]\n",
    "        index1 = np.where((peak_lost[:, 0] <= rt1 + rt_error) & (peak_lost[:, 0] >= rt1 - rt_error)\n",
    "                          & (peak_lost[:, 1] <= mz1 + mz_error) & (peak_lost[:, 1] >= mz1 - mz_error))\n",
    "        peak = np.mean(peak_lost[index1], axis=0).tolist()\n",
    "        peak = [round(peak[0],2),round(peak[1],4)]\n",
    "        peak_lost = np.delete(peak_lost, index1, axis=0)\n",
    "        peak_ref.append(peak)\n",
    "        print(f'\\r  {len(pair)}                        ', end='')\n",
    "    \n",
    "    return np.array(peak_ref)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "#     path = r'D:\\TOF-Ms DATA\\HYH-MZML\\混合实验\\*.mzML'\n",
    "#     files_mzml = glob(path)\n",
    "#     pool = Pool(processes = 5)\n",
    "#     for file in files_mzml:\n",
    "#         pool.apply_async(multi_process,args=(file,))\n",
    "#     print('Finished')\n",
    "#     pool.close()\n",
    "#     pool.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'E:\\YYN\\*.mzML'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mzml = glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\YYN\\\\LC-2021-11-04-pos-QCQC-1.mzML'"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_mzml[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reading files finished! Total time: 64.7 s           "
     ]
    }
   ],
   "source": [
    "ms1,ms2,lockspray = sep_scans(files_mzml[-1],'Waters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Concat finished, Consumed time: 227.99 s                     "
     ]
    }
   ],
   "source": [
    "df1 = gen_df_raw(ms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.027</th>\n",
       "      <th>0.034</th>\n",
       "      <th>0.041</th>\n",
       "      <th>0.048</th>\n",
       "      <th>0.056</th>\n",
       "      <th>0.063</th>\n",
       "      <th>0.070</th>\n",
       "      <th>0.077</th>\n",
       "      <th>0.084</th>\n",
       "      <th>0.091</th>\n",
       "      <th>...</th>\n",
       "      <th>24.933</th>\n",
       "      <th>24.940</th>\n",
       "      <th>24.947</th>\n",
       "      <th>24.954</th>\n",
       "      <th>24.961</th>\n",
       "      <th>24.968</th>\n",
       "      <th>24.975</th>\n",
       "      <th>24.983</th>\n",
       "      <th>24.990</th>\n",
       "      <th>24.997</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49.9993</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0007</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0022</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0036</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0051</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.9675</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.9740</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.9805</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.9869</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999.9934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240024 rows × 3408 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.027   0.034   0.041   0.048   0.056   0.063   0.070   0.077   \\\n",
       "49.9993      0.0     0.0     7.0     8.0     0.0     0.0     0.0     0.0   \n",
       "50.0007      0.0     0.0     8.0    11.0     0.0     0.0     0.0     0.0   \n",
       "50.0022      0.0     0.0    10.0     1.0     0.0     0.0     1.0     0.0   \n",
       "50.0036      0.0     0.0     0.0     0.0     0.0     0.0    15.0    12.0   \n",
       "50.0051      0.0     0.0     0.0     0.0     0.0     0.0    16.0    20.0   \n",
       "...          ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "999.9675     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "999.9740     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "999.9805     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "999.9869     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "999.9934     0.0     0.0     0.0     7.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "          0.084   0.091   ...  24.933  24.940  24.947  24.954  24.961  24.968  \\\n",
       "49.9993      0.0     0.0  ...     9.0     0.0    13.0    24.0    29.0     0.0   \n",
       "50.0007      0.0     0.0  ...     6.0    12.0     2.0    14.0    49.0     9.0   \n",
       "50.0022      0.0     0.0  ...     0.0    30.0     0.0     0.0    38.0    18.0   \n",
       "50.0036      0.0     0.0  ...     0.0    18.0    14.0    14.0    23.0    15.0   \n",
       "50.0051      0.0     0.0  ...     0.0     0.0    16.0    23.0    18.0    12.0   \n",
       "...          ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "999.9675     0.0     0.0  ...    29.0     8.0     4.0   104.0    51.0     2.0   \n",
       "999.9740     0.0     0.0  ...    24.0    18.0    10.0   100.0    68.0     7.0   \n",
       "999.9805     0.0     0.0  ...    14.0     9.0     6.0    15.0    62.0    13.0   \n",
       "999.9869     0.0     0.0  ...    21.0     0.0     3.0     7.0    35.0    14.0   \n",
       "999.9934     0.0     0.0  ...    28.0     0.0    18.0    17.0    45.0    31.0   \n",
       "\n",
       "          24.975  24.983  24.990  24.997  \n",
       "49.9993      0.0    12.0     0.0    32.0  \n",
       "50.0007      0.0    23.0     8.0     0.0  \n",
       "50.0022      0.0    23.0     9.0     0.0  \n",
       "50.0036     10.0     9.0     0.0     0.0  \n",
       "50.0051     35.0    24.0     4.0     0.0  \n",
       "...          ...     ...     ...     ...  \n",
       "999.9675     8.0    40.0   296.0    24.0  \n",
       "999.9740     0.0     4.0    49.0    51.0  \n",
       "999.9805    11.0     0.0    29.0    43.0  \n",
       "999.9869    12.0     4.0    46.0    24.0  \n",
       "999.9934     0.0    30.0    57.0    45.0  \n",
       "\n",
       "[240024 rows x 3408 columns]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " finding peaks...100.0%                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              "
     ]
    }
   ],
   "source": [
    "peak_all = peak_picking(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 生成Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1,ms2,lockspray = sep_scans(files_mzml[-1],'Waters')\n",
    "df1 = gen_df_raw(ms1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 做峰提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_all = peak_picking(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 做峰检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing peaks...16589/16589                        "
     ]
    }
   ],
   "source": [
    "peak_selected = peak_checking(peak_all,df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.做alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E:\\\\YYN\\\\LC-2021-11-04-PoolDay0-1.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-PoolDay0-2.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-PoolDay0-3.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-PoolFieldblank-1.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-PoolFieldblank-2.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-PoolFieldblank-3.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-pos-Methanol-1.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-pos-Methanol-2.xlsx',\n",
       " 'E:\\\\YYN\\\\LC-2021-11-04-pos-Methanol-3.xlsx']"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_excel = glob(r'E:\\YYN\\*.xlsx')\n",
    "files_excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " for 9 files, generating alignment results... "
     ]
    }
   ],
   "source": [
    "peak_alignment(files_excel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 拿到索引，重新做积分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(peak_ref,columns = ['rt','mz']).to_excel(r'E:\\YYN\\peak_ref.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_excel = glob(r'E:\\YYN\\*.xlsx')\n",
    "files_excel=files_excel[2:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolday0=(concat_alignment(files_excel)+0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LC-2021-11-04-PoolDay0-1</th>\n",
       "      <th>LC-2021-11-04-PoolDay0-2</th>\n",
       "      <th>LC-2021-11-04-PoolDay0-3</th>\n",
       "      <th>LC-2021-11-04-PoolFieldblank-1</th>\n",
       "      <th>LC-2021-11-04-PoolFieldblank-2</th>\n",
       "      <th>LC-2021-11-04-PoolFieldblank-3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9.8_520.6968</th>\n",
       "      <td>169.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>143.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.08_358.4864</th>\n",
       "      <td>392.1</td>\n",
       "      <td>377.1</td>\n",
       "      <td>430.1</td>\n",
       "      <td>445.1</td>\n",
       "      <td>443.1</td>\n",
       "      <td>359.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8.83_120.2027</th>\n",
       "      <td>140.1</td>\n",
       "      <td>159.1</td>\n",
       "      <td>116.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>9.1</td>\n",
       "      <td>14.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.23_172.2051</th>\n",
       "      <td>111.1</td>\n",
       "      <td>129.1</td>\n",
       "      <td>99.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.77_280.4268</th>\n",
       "      <td>85.1</td>\n",
       "      <td>78.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>111.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>66.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16.31_275.3506</th>\n",
       "      <td>45.1</td>\n",
       "      <td>32.1</td>\n",
       "      <td>47.1</td>\n",
       "      <td>38.1</td>\n",
       "      <td>37.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.07_359.4676</th>\n",
       "      <td>171.1</td>\n",
       "      <td>169.1</td>\n",
       "      <td>164.1</td>\n",
       "      <td>178.1</td>\n",
       "      <td>203.1</td>\n",
       "      <td>211.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19.07_366.3578</th>\n",
       "      <td>2129.1</td>\n",
       "      <td>2256.1</td>\n",
       "      <td>2093.1</td>\n",
       "      <td>1247.1</td>\n",
       "      <td>1257.1</td>\n",
       "      <td>1272.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21.21_850.2381</th>\n",
       "      <td>2346.1</td>\n",
       "      <td>2247.1</td>\n",
       "      <td>2355.1</td>\n",
       "      <td>4080.1</td>\n",
       "      <td>3114.1</td>\n",
       "      <td>2441.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18.19_531.5361</th>\n",
       "      <td>1262.1</td>\n",
       "      <td>1466.1</td>\n",
       "      <td>948.1</td>\n",
       "      <td>28.1</td>\n",
       "      <td>27.1</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2391 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                LC-2021-11-04-PoolDay0-1  LC-2021-11-04-PoolDay0-2  \\\n",
       "9.8_520.6968                       169.1                     170.1   \n",
       "19.08_358.4864                     392.1                     377.1   \n",
       "8.83_120.2027                      140.1                     159.1   \n",
       "6.23_172.2051                      111.1                     129.1   \n",
       "19.77_280.4268                      85.1                      78.1   \n",
       "...                                  ...                       ...   \n",
       "16.31_275.3506                      45.1                      32.1   \n",
       "19.07_359.4676                     171.1                     169.1   \n",
       "19.07_366.3578                    2129.1                    2256.1   \n",
       "21.21_850.2381                    2346.1                    2247.1   \n",
       "18.19_531.5361                    1262.1                    1466.1   \n",
       "\n",
       "                LC-2021-11-04-PoolDay0-3  LC-2021-11-04-PoolFieldblank-1  \\\n",
       "9.8_520.6968                       143.1                             8.1   \n",
       "19.08_358.4864                     430.1                           445.1   \n",
       "8.83_120.2027                      116.1                             5.1   \n",
       "6.23_172.2051                       99.1                             3.1   \n",
       "19.77_280.4268                      33.1                           111.1   \n",
       "...                                  ...                             ...   \n",
       "16.31_275.3506                      47.1                            38.1   \n",
       "19.07_359.4676                     164.1                           178.1   \n",
       "19.07_366.3578                    2093.1                          1247.1   \n",
       "21.21_850.2381                    2355.1                          4080.1   \n",
       "18.19_531.5361                     948.1                            28.1   \n",
       "\n",
       "                LC-2021-11-04-PoolFieldblank-2  LC-2021-11-04-PoolFieldblank-3  \n",
       "9.8_520.6968                               7.1                             4.1  \n",
       "19.08_358.4864                           443.1                           359.1  \n",
       "8.83_120.2027                              9.1                            14.1  \n",
       "6.23_172.2051                              3.1                             1.1  \n",
       "19.77_280.4268                            11.1                            66.1  \n",
       "...                                        ...                             ...  \n",
       "16.31_275.3506                            37.1                            22.1  \n",
       "19.07_359.4676                           203.1                           211.1  \n",
       "19.07_366.3578                          1257.1                          1272.1  \n",
       "21.21_850.2381                          3114.1                          2441.1  \n",
       "18.19_531.5361                            27.1                            22.1  \n",
       "\n",
       "[2391 rows x 6 columns]"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poolday1 = poolday0[poolday0.columns[:6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolday1[poolday1['LC-2021-11-04-PoolDay0-1']>10*poolday1['LC-2021-11-04-PoolFieldblank-1']].to_excel(r'E:\\YYN\\final_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "poolday0_area = poolday1[poolday1.columns[:3]].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_blk = poolday1[poolday1.columns[3:6]].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.8_520.6968       160.766667\n",
       "19.08_358.4864     399.766667\n",
       "8.83_120.2027      138.433333\n",
       "6.23_172.2051      113.100000\n",
       "19.77_280.4268      65.433333\n",
       "                     ...     \n",
       "16.31_275.3506      41.433333\n",
       "19.07_359.4676     168.100000\n",
       "19.07_366.3578    2159.433333\n",
       "21.21_850.2381    2316.100000\n",
       "18.19_531.5361    1225.433333\n",
       "Length: 2391, dtype: float64"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poolday0_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result =pd.DataFrame(np.array([poolday0_area,f_blk]).T,columns = ['poolday0','field blank'],index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>poolday0</th>\n",
       "      <th>field blank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1215</th>\n",
       "      <td>5125.100000</td>\n",
       "      <td>3.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>39519.100000</td>\n",
       "      <td>32.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>34481.100000</td>\n",
       "      <td>26.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>64993.766667</td>\n",
       "      <td>44.766667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          poolday0  field blank\n",
       "1215   5125.100000     3.766667\n",
       "1387  39519.100000    32.433333\n",
       "1396  34481.100000    26.433333\n",
       "1427  64993.766667    44.766667"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['poolday0']>1000*result['field blank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
